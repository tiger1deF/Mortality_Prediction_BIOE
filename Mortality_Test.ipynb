{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3821345-b5a6-44d3-97c3-ecd8ff30cc02",
   "metadata": {},
   "source": [
    "# Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8269e45e-916a-4ded-ae76-ca7a4588842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/defrondeville.c/miniconda3/envs/LLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "import sys\n",
    "import pytorch_lightning as plit\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, normalize, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import auc as skauc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import torch.nn as nn\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import argparse\n",
    "import dateutil\n",
    "\n",
    "# Torch device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# K-fold validation\n",
    "n_splits = 5  \n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5b0c7-2982-4fc3-821e-6b6da93ab655",
   "metadata": {},
   "source": [
    "# Feature Analysis + Filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53be1c2e-3ca1-418f-bbdc-8b4fccbdd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(features_scaled, feature_names, y, k=5, top_n_features=10, return_top_features=False):\n",
    "    \"\"\"\n",
    "    Analyze features using SHAP values with different methods and visualize the results.\n",
    "    \n",
    "    :param features_scaled: Scaled feature numpy array\n",
    "    :param feature_names: List of feature names\n",
    "    :param y: True labels\n",
    "    :param k: Number of clusters for K-means (unused in this version)\n",
    "    :param top_n_features: Number of top features to retain based on Linear Regression\n",
    "    :param return_top_features: Whether to return only the top N features\n",
    "    \"\"\"\n",
    "    # Trains XGBoost classifier\n",
    "    tree_model = XGBClassifier()\n",
    "    tree_model.fit(features_scaled, y)\n",
    "\n",
    "    sampled_features = shap.sample(features_scaled, 300)\n",
    "    \n",
    "    # Initialize SHAP explainers\n",
    "    explainer_tree = shap.TreeExplainer(tree_model)\n",
    "\n",
    "    # Compute SHAP values using different methods\n",
    "    shap_values_tree = explainer_tree.shap_values(sampled_features)\n",
    "\n",
    "    # Visualize SHAP values using summary plot\n",
    "    shap.summary_plot(shap_values_tree, sampled_features, feature_names=feature_names)\n",
    "\n",
    "    # Print feature importance based on SHAP\n",
    "    feature_importance_shap_tree = np.abs(shap_values_tree).mean(axis=0)\n",
    "    feature_importance_shap_tree = pd.Series(feature_importance_shap_tree, index=feature_names).sort_values(ascending=False)\n",
    "    print(\"\\nFeature importance based on SHAP TreeExplainer:\")\n",
    "    print(feature_importance_shap_tree.head(top_n_features))\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Perform PCA to extract top top_n_features/2 components\n",
    "    pca = PCA(n_components=top_n_features // 2)\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    feature_names_pca = [f\"PCA_{i+1}\" for i in range(features_pca.shape[1])]\n",
    "    \n",
    "    # Map PCA components to original feature names\n",
    "    pca_component_names = [feature_names[j] for j in pca.components_.argmax(axis=1)]\n",
    "    print(\"Top features selected by PCA:\")\n",
    "    print(pca_component_names)\n",
    "    \n",
    "    # Linear Regression on top_n_features/2 features for feature importance\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(features_scaled[:, :top_n_features // 2], y)\n",
    "    feature_importance_lr = pd.Series(np.abs(lr.coef_), index=feature_names[:top_n_features // 2]).sort_values(ascending=False)\n",
    "    print(\"Top features selected by Linear Regression:\")\n",
    "    print(feature_importance_lr.index.tolist())\n",
    "    \n",
    "    # Combine top PCA features and top LR features\n",
    "    combined_feature_names = []\n",
    "    combined_features = []\n",
    "\n",
    "    # Add PCA components with original feature names\n",
    "    for pca_component, pca_name in zip(features_pca.T, pca_component_names):\n",
    "        combined_features.append(pca_component)\n",
    "        combined_feature_names.append(pca_name)\n",
    "\n",
    "    # Add top LR features that are not already selected by PCA\n",
    "    for feature_name in feature_importance_lr.index:\n",
    "        if feature_name not in combined_feature_names:\n",
    "            combined_features.append(features_scaled[:, feature_names.index(feature_name)])\n",
    "            combined_feature_names.append(feature_name)\n",
    "    \n",
    "    # Plotting PCA contributions\n",
    "    plt.figure(figsize=(6, 10))\n",
    "    sorted_pca_indices = np.argsort(pca.explained_variance_ratio_[:len(pca_component_names)])[::-1]  # Sorting indices in descending order\n",
    "    sorted_pca_component_names = [pca_component_names[i] for i in sorted_pca_indices]\n",
    "    sorted_explained_variance_ratio = [pca.explained_variance_ratio_[i] for i in sorted_pca_indices]\n",
    "    plt.barh(range(len(sorted_pca_component_names)), sorted_explained_variance_ratio, color='blue')\n",
    "    plt.xlabel('Explained Variance Ratio')\n",
    "    plt.yticks(range(len(sorted_pca_component_names)), sorted_pca_component_names)\n",
    "    plt.title('PCA Contribution')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have largest values at the top\n",
    "    plt.show()\n",
    "    \n",
    "    # Plotting LR contributions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sorted_lr_indices = feature_importance_lr.abs().argsort()[::-1]  # Sorting indices in descending order of absolute coefficient values\n",
    "    sorted_feature_importance_lr = feature_importance_lr[sorted_lr_indices]\n",
    "    sorted_feature_names_lr = sorted_feature_importance_lr.index\n",
    "    plt.barh(range(len(sorted_feature_importance_lr)), sorted_feature_importance_lr.values, color='blue')\n",
    "    plt.xlabel('Absolute Coefficient Value (log scale)')\n",
    "    plt.xscale('log')  # Log scale x-axis for better visualization\n",
    "    plt.yticks(range(len(sorted_feature_importance_lr)), sorted_feature_names_lr)\n",
    "    plt.title('Linear Regression Contribution')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have largest values at the top\n",
    "    plt.show()\n",
    "        \n",
    "    return combined_feature_names\n",
    "def process_data_chunk_wrapper(chunk, id_conversion):\n",
    "    \"\"\"\n",
    "    Calls process_data_chunk with the given chunk and id_conversion.\n",
    "    \"\"\"\n",
    "    return process_data_chunk(chunk, id_conversion)\n",
    "    \n",
    "# Filters features based on their presence in the final patient dataset\n",
    "def filter_features_on_presence(feature_df, feature_threshold =0.5, patient_threshold = 0.5):\n",
    "    presence = feature_df.notna().mean()\n",
    "    filtered_columns = presence[presence > feature_threshold].index\n",
    "    feature_df = feature_df[filtered_columns]\n",
    "    \n",
    "    # Calculate the minimum number of non-NaN/None values required per row based on the proportion\n",
    "    min_non_nan_count = int(np.ceil(patient_threshold * feature_df.shape[1]))\n",
    "    \n",
    "    # Filter rows based on the calculated minimum count of non-NaN/None values\n",
    "    filtered_df = feature_df.dropna(axis=0, thresh=min_non_nan_count)\n",
    "    \n",
    "    return filtered_df\n",
    "    \n",
    "def filter_chart_data_optimized(chart_data, admissions_data, max_hours=48, chunks=4):\n",
    "    # Ensure datetime format\n",
    "    chart_data['CHARTTIME'] = pd.to_datetime(chart_data['CHARTTIME'], errors='coerce')\n",
    "    admissions_data['ADMITTIME'] = pd.to_datetime(admissions_data['ADMITTIME'], errors='coerce')\n",
    "\n",
    "    # Determine chunk size\n",
    "    chunk_size = len(chart_data) // chunks\n",
    "    filtered_data_list = []\n",
    "\n",
    "    for i in range(chunks):\n",
    "        # Calculate start and end index for each chunk\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size if i < chunks - 1 else len(chart_data)\n",
    "        chart_data_chunk = chart_data.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Merge with admissions_data\n",
    "        merged_data = chart_data_chunk.merge(admissions_data[['SUBJECT_ID', 'ADMITTIME']],\n",
    "                                             on='SUBJECT_ID', how='inner')\n",
    "        # Calculate time difference in hours\n",
    "        merged_data['time_diff'] = (merged_data['CHARTTIME'] - merged_data['ADMITTIME']).dt.total_seconds() / 3600\n",
    "\n",
    "        # Filter based on time_diff\n",
    "        filtered_chunk = merged_data.query('0 <= time_diff <= @max_hours').drop(columns=['ADMITTIME', 'time_diff'])\n",
    "        filtered_data_list.append(filtered_chunk)\n",
    "\n",
    "    # Concatenate all filtered chunks\n",
    "    filtered_data = pd.concat(filtered_data_list, ignore_index=True)\n",
    "    \n",
    "    return filtered_data\n",
    "    \n",
    "def extract_features_from_chart_data(chart_data, id_conversion, num_processes=16, min_non_nan_proportion=0.01):\n",
    "    \"\"\"\n",
    "    Extract features from chart data using multiprocessing.\n",
    "    \"\"\"\n",
    "    # Split data into chunks\n",
    "    chunks = np.array_split(chart_data, num_processes)\n",
    "    \n",
    "    # Prepare arguments for each chunk processing\n",
    "    args = [(chunk, id_conversion) for chunk in chunks]\n",
    "    \n",
    "    # Process each chunk in parallel\n",
    "    with Pool(num_processes) as pool:\n",
    "        results = pool.starmap(process_data_chunk_wrapper, args)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    combined = pd.concat(results, axis=0).reset_index(drop=True).groupby('SUBJECT_ID').first()\n",
    "    \n",
    "    # Determine the threshold number of non-NaN/non-None values required per column\n",
    "    threshold_count = int(min_non_nan_proportion * len(combined))\n",
    "    \n",
    "    # Drop columns where the number of non-NaN/non-None values is less than the threshold\n",
    "    filtered_combined = combined.dropna(axis=1, thresh=threshold_count, inplace=False)\n",
    "    \n",
    "    return filtered_combined \n",
    "    \n",
    "def process_data_chunk(chunk, id_conversion, value_pick='last'):\n",
    "    \"\"\"\n",
    "    Extract features from a chunk of data, preserving SUBJECT_ID.\n",
    "    Args:\n",
    "        - chunk (DataFrame): A chunk of the filtered chart data, preserving SUBJECT_ID.\n",
    "        - id_conversion (dict): A dictionary for converting item IDs to feature names.\n",
    "        - value_pick (str): Determines whether to pick the 'first' or 'last' non-empty value for each feature.\n",
    "    Returns:\n",
    "        - DataFrame with extracted features for the chunk, including SUBJECT_ID.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure SUBJECT_ID is considered during processing\n",
    "    chunk['FEATURE_NAME'] = chunk['ITEMID'].map(id_conversion).fillna('Unknown')\n",
    "\n",
    "    # Define aggregation function based on value_pick parameter\n",
    "    if value_pick == 'first':\n",
    "        value_func = lambda x: x.dropna().iloc[0] if not x.dropna().empty else None\n",
    "    elif value_pick == 'last':\n",
    "        value_func = lambda x: x.dropna().iloc[-1] if not x.dropna().empty else None\n",
    "    elif value_pick == 'mean':\n",
    "        value_func = lambda x: x.dropna().iloc[:] if not x.dropna().empty() else None\n",
    "    else:\n",
    "        raise ValueError(\"value_pick must be 'first' or 'last' or 'mean'\")\n",
    "\n",
    "    agg_funcs = {'VALUE': value_func}\n",
    "    features = chunk.groupby(['SUBJECT_ID', 'FEATURE_NAME']).agg(agg_funcs).unstack()\n",
    "\n",
    "    # Flatten Multi-Index columns\n",
    "    features.columns = [f'{i}_{j}' for i, j in features.columns]\n",
    "    return features.reset_index()\n",
    "    \n",
    "def filter_patients_within_timeframe(data, max_hours):\n",
    "\n",
    "    max_seconds = max_hours * 3600\n",
    "    # Mark patients to keep based on the condition\n",
    "    keep_patients = []\n",
    "    for i, row in data.iterrows():\n",
    "        if pd.notnull(row['DEATHTIME']):\n",
    "            time_diff = (row['DEATHTIME'] - row['ADMITTIME']).total_seconds()\n",
    "        else:\n",
    "            time_diff = (row['DISCHTIME'] - row['ADMITTIME']).total_seconds()\n",
    "        \n",
    "        if time_diff > max_seconds:\n",
    "            keep_patients.append(row['SUBJECT_ID'])\n",
    "    \n",
    "    return keep_patients\n",
    "    \n",
    "def process_data_with_timeframe_filtering(admit_times, discharge_times, death_times, filter_within_timeframe, max_hours, subject_ids):\n",
    "    death_labels = []\n",
    "    hospital_stay = []\n",
    "    filtered_ids = []\n",
    "    \n",
    "    for admit, dis, death, subject_id in tqdm.tqdm(zip(admit_times, discharge_times, death_times, subject_ids), total=len(admit_times), desc='Processing mortality'):\n",
    "        within_timeframe = False\n",
    "        death_time = (death - admit).total_seconds() if pd.notnull(death) else None\n",
    "        discharge_time = (dis - admit).total_seconds() if pd.notnull(dis) else None\n",
    "        \n",
    "        if death_time and death_time <= max_hours * 3600:\n",
    "            within_timeframe = True\n",
    "        elif discharge_time and discharge_time <= max_hours * 3600:\n",
    "            within_timeframe = True\n",
    "        \n",
    "        if filter_within_timeframe and within_timeframe:\n",
    "            continue  # Skip this patient\n",
    "        \n",
    "        # Add the patient's data\n",
    "        hospital_stay.append(death_time if death_time else discharge_time)\n",
    "        death_labels.append(1 if death_time else 0)\n",
    "        filtered_ids.append(subject_id)\n",
    "    \n",
    "    return death_labels, hospital_stay, filtered_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f68dd-dc5a-4baf-b4d3-ed56f6601cfb",
   "metadata": {},
   "source": [
    "# Model Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc17847a-9e60-4924-9df7-6b8467ea2cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for equalizing data\n",
    "def equalize_data(data, labels):\n",
    "    data[\"Death\"] = labels\n",
    "    data = data.sample(frac = 1) \n",
    "    true_data = data[data[\"Death\"] == 1]\n",
    "    false_data = data[data[\"Death\"] == 0]\n",
    "    true_length, false_length = len(true_data), len(false_data)\n",
    "    \n",
    "    # Equalizes labels\n",
    "    if true_length > false_length:\n",
    "        sample_fraction = false_length / true_length\n",
    "        true_data = true_data.sample(frac = sample_fraction)\n",
    "    else:\n",
    "        sample_fraction = true_length / false_length\n",
    "        false_data = false_data.sample(frac = sample_fraction)\n",
    "        \n",
    "    # Recombines data\n",
    "    data = pd.concat((true_data, false_data), axis = 0)\n",
    "    labels = list(data['Death'])\n",
    "    features = data.drop(columns = ['Death'])\n",
    "    return features, labels\n",
    "\n",
    "# Obtains all filtered features and patients\n",
    "def obtain_features(format_str = \"%Y-%m-%d %H:%M:%S\",\n",
    "                max_hours =6, patient_threshold = 0.5, feature_threshold = 0.5,\n",
    "                filter_within_timeframe = True):\n",
    "\n",
    "    # Sets global variables to facilitate multiprocessing functions\n",
    "    global id_conversion, num_hours\n",
    " \n",
    "    # Obtain discharge/death/admission times for label processing\n",
    "    data = pd.read_csv('dataset/ADMISSIONS.csv')\n",
    "    chart_data = pd.read_csv('dataset/CHARTEVENTS.csv',)\n",
    "    patient_data = pd.read_csv('dataset/PATIENTS.csv')[['SUBJECT_ID', 'DOB']]\n",
    "    patient_data['DOB'] = pd.to_datetime(patient_data['DOB'])\n",
    "\n",
    "    #micro_data = pd.read_csv('dataset/MICROBIOLOGYEVENTS.csv')\n",
    "    #note_data = pd.read_csv('./dataset/NOTEEVENTS.csv')\n",
    "    #prescription_data = pd.read_csv('dataset/PRESCRIPTIONS.csv')\n",
    "\n",
    "    # Conversion dict for item IDs \n",
    "    conversion_data = pd.read_csv('dataset/D_ITEMS.csv')\n",
    "\n",
    "    # Extract time-related discharge, death, and admit values and converts them to float\n",
    "    data['ADMITTIME'] = pd.to_datetime(data['ADMITTIME'],)\n",
    "    data['DISCHTIME'] = pd.to_datetime(data['DISCHTIME'])\n",
    "    data['DEATHTIME'] = pd.to_datetime(data['DEATHTIME'],)\n",
    "    \n",
    "    # New logic to filter patients based on discharge/death time within the timeframe\n",
    "    admit_times = data['ADMITTIME']\n",
    "    mapping_discharge = {data['SUBJECT_ID']: data['ADMITTIME'] for _, data in data.iterrows()}\n",
    "    discharge_times = data['DISCHTIME']\n",
    "    death_times = data['DEATHTIME']\n",
    "\n",
    "    admit_times = data['ADMITTIME']\n",
    "    discharge_times = data['DISCHTIME']\n",
    "    death_times = data['DEATHTIME']\n",
    "    \n",
    "    # Make sure 'ADMITTIME' and 'DOB' are in datetime format\n",
    "    data['ADMITTIME'] = pd.to_datetime(data['ADMITTIME'])\n",
    "    patient_data['DOB'] = pd.to_datetime(patient_data['DOB'])\n",
    "\n",
    "    # Calculate age using relativedelta\n",
    "    def calculate_age(row):\n",
    "        return dateutil.relativedelta.relativedelta(row['ADMITTIME'], row['DOB']).years\n",
    "\n",
    "    # Merge 'data' and 'patient_data' to get 'DOB' and 'ADMITTIME' in the same DataFrame for calculation\n",
    "    patient_age_df = data.merge(patient_data, on='SUBJECT_ID', how='left')\n",
    "    patient_age_df['AGE'] = [None for _ in range(len(patient_age_df))]\n",
    "    patient_age_df.drop_duplicates(inplace = True)\n",
    "    \n",
    "    # Apply the age calculation\n",
    "    patient_age_df['AGE'] = patient_age_df.apply(calculate_age, axis=1)\n",
    "    patient_age_df = patient_age_df[['SUBJECT_ID', 'AGE']].copy()\n",
    "    \n",
    "    # Filter based on discharge/death time within the timeframe and other logic...\n",
    "    death_labels, hospital_stay, filtered_ids = process_data_with_timeframe_filtering(\n",
    "        admit_times, discharge_times, death_times, filter_within_timeframe, max_hours, subject_ids = data['SUBJECT_ID'])\n",
    "    \n",
    "    # Ensure filtered_ids is a list of IDs that should be kept\n",
    "    chart_data = chart_data[chart_data['SUBJECT_ID'].isin(filtered_ids)]\n",
    "    \n",
    "    mortality_labels = pd.DataFrame({\n",
    "        'SUBJECT_ID': filtered_ids,\n",
    "        'Death':death_labels\n",
    "    })\n",
    "    \n",
    "    # Convert ITEMID to biomedical statistic labels\n",
    "    id_conversion = {row['ITEMID']: row['LABEL'] for _, row in conversion_data.iterrows()}\n",
    "    \n",
    "    # Convert CHARTTIME to datetime\n",
    "    chart_data['CHARTTIME'] = pd.to_datetime(chart_data['CHARTTIME'])\n",
    "\n",
    "    # Convert times outside the loop\n",
    "    chart_data['CHARTTIME'] = pd.to_datetime(chart_data['CHARTTIME'])\n",
    "\n",
    "    # Properly adds admit times to df\n",
    "    print('Adding times...')\n",
    "    chart_admit = [mapping_discharge[subject_id] if subject_id in mapping_discharge else None for subject_id in chart_data['SUBJECT_ID']]\n",
    "    chart_data['admit_time'] = chart_admit\n",
    "    chart_data['admit_time'] = pd.to_datetime(chart_data['admit_time'], errors = 'coerce')\n",
    "    chart_data = chart_data.dropna(subset = ['admit_time'])\n",
    "\n",
    "    # Ensure dateti2me format and handle NaN values\n",
    "    chart_data['CHARTTIME'] = pd.to_datetime(chart_data['CHARTTIME'], errors='coerce')\n",
    "    chart_data = chart_data.dropna(subset=['CHARTTIME'])\n",
    "\n",
    "    # Filter chart_data to include only entries within the specified time frame\n",
    "    print('Filtering data...')\n",
    "    chart_data = filter_chart_data_optimized(chart_data, data, max_hours=max_hours)\n",
    "    \n",
    "    print('Obtaining features...')\n",
    "    feature_df = extract_features_from_chart_data(chart_data, id_conversion, num_processes=16)\n",
    "  \n",
    "    filtered_feature_df = filter_features_on_presence(feature_df, feature_threshold = feature_threshold, patient_threshold = patient_threshold)\n",
    "\n",
    "    # Ensure SUBJECT_ID is a column if it's not already\n",
    "    patient_data_filtered = filtered_feature_df.reset_index(inplace=False)\n",
    "   \n",
    "    # Merge with mortality labels\n",
    "    patient_data = patient_data_filtered.merge(mortality_labels, on='SUBJECT_ID', how='left')\n",
    "\n",
    "    # Merge with ages\n",
    "    patient_data = patient_age_df.merge(patient_data, on='SUBJECT_ID',)\n",
    "    \n",
    "    # Drops columns with all NaN values and duplicate columns/rows\n",
    "    patient_data = patient_data.dropna(axis=1, how='all')\n",
    "    cols_to_consider = patient_data.columns.tolist()\n",
    "    cols_to_consider.remove('AGE')\n",
    "    cols_to_consider.remove('SUBJECT_ID')\n",
    "    patient_data = patient_data.drop_duplicates(subset=cols_to_consider, keep='first')\n",
    "\n",
    "    # Performs encoding\n",
    "    labellers = {}\n",
    "    string_columns_count = 0  # Counter for columns with string values\n",
    "    for column in tqdm.tqdm(patient_data.columns, total = len(patient_data.columns), desc = 'Labelling data...'):\n",
    "\n",
    "        # Check if there are any string values in the column\n",
    "        if patient_data[column].apply(lambda x: isinstance(x, str)).any():\n",
    "\n",
    "            try:\n",
    "                # Increment the counter as this column contains string data\n",
    "                string_columns_count += 1\n",
    "\n",
    "                # Fill NaN values with a placeholder\n",
    "                patient_data[column] = patient_data[column].fillna('missing')\n",
    "\n",
    "                # Initialize LabelEncoder and transform values\n",
    "                le = LabelEncoder()\n",
    "                patient_data[column] = le.fit_transform(patient_data[column])\n",
    "                labellers[column] = le\n",
    "\n",
    "                # Identify the encoded value for 'missing' placeholder\n",
    "                missing_label = le.transform(['missing'])[0]\n",
    "\n",
    "                # Replace the 'missing' encoded value with NaN in the dataframe\n",
    "                patient_data[column] = patient_data[column].replace(missing_label, None)\n",
    "            except:\n",
    "                patient_data[column] = patient_data[column].apply(lambda x: None if isinstance(x, str) else x)\n",
    "\n",
    "    # Number of filtered patients\n",
    "    print(f'{len(patient_data.columns)} features remaining after final filtration')\n",
    "    print(f'{len(patient_data)} patients remaining after final filtration')\n",
    "    \n",
    "    # Removes certain features \n",
    "    features_to_remove = ['VALUE_Education Learner', 'VALUE_Education Method', 'VALUE_Education Readiness', 'VALUE_Education Response', \n",
    "                            'Value_Marital Status', 'VALUE_Religion', 'VALUE_Orientation', 'VALUE_Family Communication',\n",
    "                            # Potentially remove\n",
    "                            'VALUE_Code Status']\n",
    "    \n",
    "    for feature in features_to_remove:\n",
    "        try:\n",
    "            patient_data = patient_data.drop(columns = [feature])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    labels = patient_data['Death']\n",
    "    features = patient_data.drop(columns = ['Death', 'SUBJECT_ID'])\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    numpy_features = np.array([[item for item in row] for row in features.to_numpy()])\n",
    "    numpy_features = scaler.fit_transform(numpy_features)\n",
    "    imputer = MICE()\n",
    "    \n",
    "    print('Imputing...')\n",
    "    impute_start = time.perf_counter()\n",
    "    imputed_data = imputer.fit_transform(numpy_features)\n",
    "    impute_end = time.perf_counter()\n",
    "    print(f'MICE Imputation finished in {round((impute_end - impute_start)/60, 4)} minutes!')\n",
    "    features = pd.DataFrame(imputed_data, columns=features.columns, index=features.index)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "def feature_analysis(features, labels, top_features = None):\n",
    "    # Analyzes feature importance via a variety of methods and filters features if enabled\n",
    "    if top_features:\n",
    "        feature_names = analyze_features(features_scaled = features.to_numpy(), feature_names = list(features.columns), y = labels,\n",
    "                                    return_top_features = True, top_n_features = top_features,)\n",
    "        return feature_names\n",
    "    else:\n",
    "        analyze_features(features_scaled = features.to_numpy(), feature_names = list(features.columns), y = labels, return_top_features = False)\n",
    "    \n",
    "# Tests all models on the aggregated features and data\n",
    "def test_models(features, labels, max_hours = 24, feature_names = None):\n",
    "\n",
    "    if feature_names != None:\n",
    "        features = features[feature_names]\n",
    "        \n",
    "    print(f'Number of features: {len(features.columns)}')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    def k_fold_validation_for_sklearn(clf, features, \n",
    "                                      labels, \n",
    "                                      name = 'Random Forest'):\n",
    "\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        for i, (train_index, test_index) in tqdm.tqdm(enumerate(kf.split(features)), total = n_splits):\n",
    "            \n",
    "            X_train, X_test = features[train_index], features[test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "            \n",
    "            # Scale features\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Fit and predict\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "           \n",
    "            # Compute ROC curve and AUC for this fold\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "            roc_auc = skauc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            \n",
    "            # Interpolate all ROC curves at this points\n",
    "            interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "            interp_tpr[0] = 0.0\n",
    "            plt.plot(mean_fpr, interp_tpr, alpha=0.4, label = f'split {i} AUC = {round(roc_auc, 3)}')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        \n",
    "        # Calculate the mean AUC and std deviation\n",
    "        mean_auc = np.mean(aucs)\n",
    "        std_auc = np.std(aucs)\n",
    "        plt.title(f'{name}, ROC (AUC = {mean_auc:.2f} $\\pm$ {std_auc:.2f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    # Equalizes labels\n",
    "    features, labels = equalize_data(features, labels) \n",
    "    print(f'Number of processed data samples: {len(labels)}')\n",
    "    \n",
    "    # Prepare the data\n",
    "    features_array, labels = features.to_numpy(), np.array(labels)\n",
    "    \n",
    "    # Random Forest Classifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "    k_fold_validation_for_sklearn(rf_clf, features_array, labels)\n",
    "    \n",
    "    # XGBoost Classifier\n",
    "    xgb_clf = XGBClassifier(n_estimators=1000, learning_rate=0.05, use_label_encoder=False, eval_metric='logloss',\n",
    "                           name = 'XGBoost')\n",
    "    k_fold_validation_for_sklearn(xgb_clf, features_array, labels, name = 'XGBoost')\n",
    "    \n",
    "    # Custom-built network\n",
    "    class NeuralNet(plit.LightningModule):\n",
    "        def __init__(self, input_size, hidden_size=32):\n",
    "            super(NeuralNet, self).__init__()\n",
    "            \n",
    "            self.network = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size * 8), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size * 8), nn.Dropout(0.3),\n",
    "                \n",
    "                nn.Linear(hidden_size * 8, hidden_size * 4), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size * 4), nn.Dropout(0.3),\n",
    "                \n",
    "                nn.Linear(hidden_size * 4, hidden_size * 2), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size * 2), nn.Dropout(0.3),\n",
    "                \n",
    "                nn.Linear(hidden_size * 2, hidden_size), nn.ReLU(),\n",
    "                \n",
    "                nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size), nn.Dropout(0.5),\n",
    "            )\n",
    "\n",
    "            self.final_linear = nn.Linear(hidden_size, 1)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.residual_transform = nn.Linear(input_size, hidden_size)\n",
    "            \n",
    "            # L1 and L2 regularization are defined but not used as layers\n",
    "            \n",
    "        def forward(self, x):\n",
    "            residual = self.residual_transform(x)\n",
    "            out = self.network(x)\n",
    "            out += residual  # Add residual connection\n",
    "            out = self.final_linear(out)\n",
    "            out = self.sigmoid(out)\n",
    "            return out.squeeze()\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            inputs, labels = batch\n",
    "            outputs = self(inputs)\n",
    "            loss = nn.BCELoss()(outputs, labels.view(-1))  # BCELoss for binary classification\n",
    "\n",
    "            # Apply L1 and L2 regularization\n",
    "            l1_reg = torch.tensor(0.)\n",
    "            l2_reg = torch.tensor(0.)\n",
    "            for param in self.parameters():\n",
    "                l1_reg = l1_reg + torch.norm(param, 1)\n",
    "                l2_reg = l2_reg + torch.norm(param, 2)\n",
    "\n",
    "            # Regularization strengths need to be defined; for example:\n",
    "            lambda1, lambda2 = 0.005, 0.005\n",
    "            loss += lambda1 * l1_reg + lambda2 * l2_reg\n",
    "            self.log(\"train_loss\", loss)\n",
    "            return loss\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=0.001)\n",
    "            return optimizer\n",
    "    \n",
    "    def k_fold_validation_for_pytorch(model_class, features, labels, input_size):\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        plt.figure()\n",
    "        for train_index, test_index in tqdm.tqdm(kf.split(features), total = n_splits,):\n",
    "            # Prepare data\n",
    "            X_train, X_test = features[train_index], features[test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "           \n",
    "            # Convert to tensors\n",
    "            X_train_tensor, y_train_tensor = torch.FloatTensor(X_train), torch.FloatTensor(y_train)\n",
    "            X_test_tensor, y_test_tensor = torch.FloatTensor(X_test), torch.FloatTensor(y_test)\n",
    "            \n",
    "            # DataLoader setup\n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "            \n",
    "            # Model setup\n",
    "            model = model_class(input_size=input_size)\n",
    "            trainer = plit.Trainer(max_epochs=60)\n",
    "        \n",
    "            trainer.fit(model, train_loader)\n",
    "            \n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_prob = model(X_test_tensor).numpy()\n",
    "            \n",
    "            # Compute ROC curve and AUC for this fold\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "            roc_auc = skauc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            \n",
    "            interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "            interp_tpr[0] = 0.0\n",
    "            plt.plot(mean_fpr, interp_tpr, alpha=0.4)\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        mean_auc = np.mean(aucs)\n",
    "        std_auc = np.std(aucs)\n",
    "        plt.title(f'Custom Neural Network Mean ROC (AUC = {mean_auc:.2f} $\\pm$ {std_auc:.2f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"Custom Neural Network\")\n",
    "    k_fold_validation_for_pytorch(NeuralNet, features_array, labels, input_size=features.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273e9fd-d4e9-4d2e-a693-c354c4a23f1f",
   "metadata": {},
   "source": [
    "# Main Runtime + Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd5416-592b-4db0-ba8c-f284be66982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns (8,10,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "Processing mortality: 100%|██████████| 58976/58976 [00:00<00:00, 128045.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding times...\n"
     ]
    }
   ],
   "source": [
    "# Config options for mode\n",
    "hours = 6\n",
    "feature_threshold = 0.2\n",
    "patient_threshold = 0.25\n",
    "\n",
    "# Obtains features\n",
    "features, labels = obtain_features(max_hours = hours, feature_threshold = feature_threshold,\n",
    "                                   patient_threshold = patient_threshold,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aeb823-45ac-42b6-ae5d-03f246d6989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs feature analysis\n",
    "feature_names = feature_analysis(features, labels, top_features = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b370d-d887-48f8-b4da-4463838de2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests model\n",
    "print('Pre-selected features: {[print(i) for i in feature_names]}')\n",
    "test_models(features, labels, max_hours = hours, feature_names = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5878628-3d7e-46bb-96b8-8cd78739449e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251e1be-acbc-4610-9c20-4ff9ace25908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ba93c-17f9-43e6-8a56-12d5f400616d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17369e44-62c2-41c9-90e8-6bcb862d4b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
