{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3821345-b5a6-44d3-97c3-ecd8ff30cc02",
   "metadata": {},
   "source": [
    "# Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8269e45e-916a-4ded-ae76-ca7a4588842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/defrondeville.c/miniconda3/envs/LLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "import sys\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as plit\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, normalize, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import auc as skauc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import torch.nn as nn\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import argparse\n",
    "import dateutil\n",
    "from collections import defaultdict\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Torch device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# K-fold validation\n",
    "n_splits = 5  \n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08674e-afb7-4d75-b594-b5d251c84a1a",
   "metadata": {},
   "source": [
    "# Autoencoder for Patient Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3d589c-b6ea-4a78-a7ad-84091f88e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_dim, 256)\n",
    "        self.encoder_fc2_mean = nn.Linear(256, latent_dim)  # Output mean\n",
    "        self.encoder_fc2_logvar = nn.Linear(256, latent_dim)  # Output log variance\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.decoder_fc2 = nn.Linear(256, input_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.encoder_fc1(x))\n",
    "        return self.encoder_fc2_mean(x), self.encoder_fc2_logvar(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = F.relu(self.decoder_fc1(z))\n",
    "        return torch.sigmoid(self.decoder_fc2(x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        z_mean, z_log_var = self.encode(x)\n",
    "\n",
    "        # Sampling\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "\n",
    "        # Decoding\n",
    "        reconstructed = self.decode(z)\n",
    "        \n",
    "        return reconstructed, z_mean, z_log_var\n",
    "    \n",
    "    def infer(self, x):\n",
    "        # Encoding\n",
    "        z_mean, _ = self.encode(x)\n",
    "        z_mean = z_mean.detach().numpy()[0]\n",
    "        return z_mean\n",
    "        \n",
    "class AutoEncoder:\n",
    "    def __init__(self, ICD = 'dataset/DIAGNOSES_ICD.csv',\n",
    "                 conversion = 'dataset/D_ICD_DIAGNOSES.csv',\n",
    "                 max_length = 100,\n",
    "                 latent_dim = 1):\n",
    "        \n",
    "        # Loads data\n",
    "        self.conversion = pd.read_csv(conversion)\n",
    "        self.ICD = pd.read_csv(ICD)\n",
    "\n",
    "        tokens = set(self.conversion['ICD9_CODE'])\n",
    "        self.code_converter = {item:i for i, item in zip(range(len(tokens)), tokens)}\n",
    "        subject_codes = defaultdict(list)\n",
    "\n",
    "        for num, row in tqdm.tqdm(self.ICD.iterrows(), total = len(self.ICD), desc = 'Creating code features'):\n",
    "            subject_id = row['SUBJECT_ID']\n",
    "            diagnoses = row['ICD9_CODE']\n",
    "            try:\n",
    "                subject_codes[subject_id].append(code_converter[diagnoses])\n",
    "            except:\n",
    "                self.code_converter[diagnoses] = len(self.code_converter)\n",
    "                subject_codes[subject_id].append(self.code_converter[diagnoses])\n",
    "                \n",
    "        subject_codes = dict(subject_codes)\n",
    "        self.subjects = list(subject_codes.keys())\n",
    "\n",
    "        # Obtains tensors\n",
    "        self.input_tensors = []\n",
    "        self.max_length = max_length\n",
    "        for num, (subject_id, codes) in tqdm.tqdm(enumerate(subject_codes.items()), total = len(subject_codes), desc = 'Padding samples'):\n",
    "            while len(codes) < max_length:\n",
    "                codes.append(0)\n",
    "            if len(codes) > max_length:\n",
    "                codes = random.sample(codes, max_length)\n",
    "            self.input_tensors.append(torch.FloatTensor(codes))\n",
    "\n",
    "        # Scales data\n",
    "        self.scaler = StandardScaler()\n",
    "        self.input_tensors = torch.FloatTensor(self.scaler.fit_transform(self.input_tensors))\n",
    "        \n",
    "        # Makes VAE\n",
    "        self.autoencoder = VAE(input_dim = self.max_length, latent_dim = latent_dim)\n",
    "\n",
    "\n",
    "    def train_VAE(self, batch_size = 32,\n",
    "                 num_epochs = 10):\n",
    "        \n",
    "        # Define the DataLoader\n",
    "        batch_size = 16\n",
    "        data_loader = DataLoader(self.input_tensors, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        def loss_function(reconstructed_x, x, mu, log_var):\n",
    "            # Reconstruction loss (MSE)\n",
    "            recon_loss = F.mse_loss(reconstructed_x, x) / x.size(0)  # Divide by batch size for stability\n",
    "            \n",
    "            # KL divergence\n",
    "            kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) / x.size(0)\n",
    "        \n",
    "            return recon_loss + kld_loss\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.autoencoder.parameters(), lr=0.0001)\n",
    "\n",
    "        # Training loop\n",
    "        self.autoencoder.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0\n",
    "            for i, data in enumerate(tqdm.tqdm(data_loader, total=len(data_loader), desc=f'Epoch {epoch}')):\n",
    "                optimizer.zero_grad()\n",
    "                outputs, mu, log_var = self.autoencoder(data)\n",
    "                loss = loss_function(outputs, data, mu, log_var)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(data_loader):.4f}')\n",
    "\n",
    "        # Obtains encoded features\n",
    "        encoded_features = []\n",
    "        for feature in tqdm.tqdm(self.input_tensors, total = len(self.input_tensors), desc = 'Encoding...'):\n",
    "            feature = self.autoencoder.infer(feature)\n",
    "            encoded_features.append(feature)\n",
    "        return encoded_features\n",
    "\n",
    "    def extract_features(self, data, scale = True):\n",
    "        data = np.array(data)\n",
    "        self.autoencoder.eval()\n",
    "        \n",
    "        if len(data) < self.max_length:    \n",
    "            while len(data) < self.max_length:\n",
    "                data = np.append(data, 0)\n",
    "                \n",
    "        elif len(data) > self.max_length:\n",
    "            data = data[self.max_length]\n",
    "\n",
    "        data = self.scaler.transform([data])\n",
    "        \n",
    "        outputs = self.autoencoder.infer(torch.FloatTensor(data)).detach().numpy()[0]\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5b0c7-2982-4fc3-821e-6b6da93ab655",
   "metadata": {},
   "source": [
    "# Feature Analysis + Filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53be1c2e-3ca1-418f-bbdc-8b4fccbdd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(features_scaled, # Feature dataframe\n",
    "                     feature_names, # Binary - on drug or not on drug EVERY DRUG NAME [i for i in features.columns if 'dose' in i] \n",
    "                     y, # Mortality labels \n",
    "                     top_n_features=10, # What features will be plotted (shap, LR, PCA) \n",
    "                     return_top_features = False ): # Return a list of filtered feature name  \n",
    "    \"\"\"\n",
    "    Analyze features using SHAP values, Linear Regression, and Random Forest, \n",
    "    select the top features from each method without duplicates, and visualize the results.\n",
    "\n",
    "    :param features_scaled: Scaled feature numpy array\n",
    "    :param feature_names: List of feature names\n",
    "    :param y: True labels\n",
    "    :param top_n_features: Number of top features to retain from each method\n",
    "    \"\"\"\n",
    "    # Trains XGBoost classifier for SHAP\n",
    "    xgb_model = XGBClassifier()\n",
    "    xgb_model.fit(features_scaled, y)\n",
    "    \n",
    "    # Trains Random Forest classifier\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(features_scaled, y)\n",
    "\n",
    "    # Linear Regression for feature importance\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(features_scaled, y)\n",
    "\n",
    "    # SHAP values\n",
    "    explainer_shap = shap.Explainer(xgb_model)\n",
    "    shap_values = explainer_shap(features_scaled)\n",
    "    sampled_features = shap.sample(features_scaled, 1000)\n",
    "    explainer_tree = shap.TreeExplainer(xgb_model)\n",
    "    shap_values_tree = explainer_tree.shap_values(sampled_features)\n",
    "\n",
    "    # Feature importance from SHAP\n",
    "    feature_importance_shap = np.abs(shap_values.values).mean(axis=0)\n",
    "    top_features_shap = np.argsort(feature_importance_shap)[-top_n_features:]\n",
    "\n",
    "    # Feature importance from RF\n",
    "    feature_importance_rf = rf_model.feature_importances_\n",
    "    top_features_rf = np.argsort(feature_importance_rf)[-top_n_features:]\n",
    "\n",
    "    # Feature importance from LR\n",
    "    feature_importance_lr = np.abs(lr_model.coef_)\n",
    "    top_features_lr = np.argsort(feature_importance_lr)[-top_n_features:]\n",
    "\n",
    "    # Visualization\n",
    "    \n",
    "    # Visualize SHAP values using summary plot\n",
    "    shap.summary_plot(shap_values_tree, sampled_features, feature_names=feature_names)\n",
    "\n",
    "    # Print feature importance based on SHAP\n",
    "    feature_importance_shap_tree = np.abs(shap_values_tree).mean(axis=0)\n",
    "    feature_importance_shap_tree = pd.Series(feature_importance_shap_tree, index=feature_names).sort_values(ascending=False)\n",
    "    print(\"\\nFeature importance based on SHAP TreeExplainer:\")\n",
    "    print(feature_importance_shap_tree.head(top_n_features))\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    # RF\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    indices = np.argsort(feature_importance_rf)[::-1][:top_n_features]\n",
    "    plt.title(\"Top Random Forest Feature Importances\")\n",
    "    plt.barh(range(top_n_features), feature_importance_rf[indices], color='b', align='center')\n",
    "    plt.yticks(range(top_n_features), [feature_names[i] for i in indices])\n",
    "    plt.xscale('log')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have largest values at the top\n",
    "\n",
    "    # LR\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    indices = np.argsort(feature_importance_lr)[::-1][:top_n_features]\n",
    "    plt.title(\"Top Linear Regression Coefficients\")\n",
    "    plt.barh(range(top_n_features), feature_importance_lr[indices], color='b', align='center')\n",
    "    plt.yticks(range(top_n_features), [feature_names[i] for i in indices])\n",
    "    plt.xscale('log')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have largest values at the top\n",
    "\n",
    "    # Combine and remove duplicates\n",
    "    combined_feature_indices = np.unique(np.concatenate((top_features_shap, top_features_rf, top_features_lr)))\n",
    "    combined_feature_names = [feature_names[i] for i in combined_feature_indices]\n",
    "\n",
    "    print(f\"Number of selected features: {len(combined_feature_names)}\")\n",
    "    print(\"Selected features:\", combined_feature_names)\n",
    "\n",
    "    if return_top_features:\n",
    "        return combined_feature_names\n",
    "\n",
    "\n",
    "def process_data_chunk_wrapper(chunk, id_conversion):\n",
    "    \"\"\"\n",
    "    Calls process_data_chunk with the given chunk and id_conversion.\n",
    "    \"\"\"\n",
    "    return process_data_chunk(chunk, id_conversion)\n",
    "    \n",
    "# Filters features based on their presence in the final patient dataset\n",
    "def filter_features_on_presence(feature_df, feature_threshold =0.5, patient_threshold = 0.5):\n",
    "    presence = feature_df.notna().mean()\n",
    "    filtered_columns = presence[presence > feature_threshold].index\n",
    "    feature_df = feature_df[filtered_columns]\n",
    "    \n",
    "    # Calculate the minimum number of non-NaN/None values required per row based on the proportion\n",
    "    min_non_nan_count = int(np.ceil(patient_threshold * feature_df.shape[1]))\n",
    "    \n",
    "    # Filter rows based on the calculated minimum count of non-NaN/None values\n",
    "    filtered_df = feature_df.dropna(axis=0, thresh=min_non_nan_count)\n",
    "    \n",
    "    return filtered_df\n",
    "    \n",
    "def filter_chart_data_optimized(chart_data, admissions_data, max_hours=48, chunks=4):\n",
    "    # Ensure datetime format\n",
    "    chart_data['CHARTTIME'] = pd.to_datetime(chart_data['CHARTTIME'], errors='coerce')\n",
    "    admissions_data['ADMITTIME'] = pd.to_datetime(admissions_data['ADMITTIME'], errors='coerce')\n",
    "\n",
    "    # Determine chunk size\n",
    "    chunk_size = len(chart_data) // chunks\n",
    "    filtered_data_list = []\n",
    "\n",
    "    for i in range(chunks):\n",
    "        # Calculate start and end index for each chunk\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size if i < chunks - 1 else len(chart_data)\n",
    "        chart_data_chunk = chart_data.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Merge with admissions_data\n",
    "        merged_data = chart_data_chunk.merge(admissions_data[['SUBJECT_ID', 'ADMITTIME']],\n",
    "                                             on='SUBJECT_ID', how='inner')\n",
    "        # Calculate time difference in hours\n",
    "        merged_data['time_diff'] = (merged_data['CHARTTIME'] - merged_data['ADMITTIME']).dt.total_seconds() / 3600\n",
    "\n",
    "        # Filter based on time_diff\n",
    "        filtered_chunk = merged_data.query('0 <= time_diff <= @max_hours').drop(columns=['ADMITTIME', 'time_diff'])\n",
    "        filtered_data_list.append(filtered_chunk)\n",
    "\n",
    "    # Concatenate all filtered chunks\n",
    "    filtered_data = pd.concat(filtered_data_list, ignore_index=True)\n",
    "    \n",
    "    return filtered_data\n",
    "    \n",
    "def extract_features_from_chart_data(chart_data, id_conversion, num_processes=16, min_non_nan_proportion=0.01):\n",
    "    \"\"\"\n",
    "    Extract features from chart data using multiprocessing.\n",
    "    \"\"\"\n",
    "    # Split data into chunks\n",
    "    chunks = np.array_split(chart_data, num_processes)\n",
    "    \n",
    "    # Prepare arguments for each chunk processing\n",
    "    args = [(chunk, id_conversion) for chunk in chunks]\n",
    "    \n",
    "    # Process each chunk in parallel\n",
    "    with Pool(num_processes) as pool:\n",
    "        results = pool.starmap(process_data_chunk_wrapper, args)\n",
    "    \n",
    "    # Combine results from all chunks\n",
    "    combined = pd.concat(results, axis=0).reset_index(drop=True).groupby('SUBJECT_ID').first()\n",
    "    \n",
    "    # Determine the threshold number of non-NaN/non-None values required per column\n",
    "    threshold_count = int(min_non_nan_proportion * len(combined))\n",
    "    \n",
    "    # Drop columns where the number of non-NaN/non-None values is less than the threshold\n",
    "    filtered_combined = combined.dropna(axis=1, thresh=threshold_count, inplace=False)\n",
    "    \n",
    "    return filtered_combined \n",
    "    \n",
    "def process_data_chunk(chunk, id_conversion, value_pick='last'):\n",
    "    \"\"\"\n",
    "    Extract features from a chunk of data, preserving SUBJECT_ID.\n",
    "    Args:\n",
    "        - chunk (DataFrame): A chunk of the filtered chart data, preserving SUBJECT_ID.\n",
    "        - id_conversion (dict): A dictionary for converting item IDs to feature names.\n",
    "        - value_pick (str): Determines whether to pick the 'first' or 'last' non-empty value for each feature.\n",
    "    Returns:\n",
    "        - DataFrame with extracted features for the chunk, including SUBJECT_ID.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure SUBJECT_ID is considered during processing\n",
    "    chunk['FEATURE_NAME'] = chunk['ITEMID'].map(id_conversion).fillna('Unknown')\n",
    "\n",
    "    # Define aggregation function based on value_pick parameter\n",
    "    if value_pick == 'first':\n",
    "        value_func = lambda x: x.dropna().iloc[0] if not x.dropna().empty else None\n",
    "    elif value_pick == 'last':\n",
    "        value_func = lambda x: x.dropna().iloc[-1] if not x.dropna().empty else None\n",
    "    elif value_pick == 'mean':\n",
    "        value_func = lambda x: x.dropna().iloc[:] if not x.dropna().empty() else None\n",
    "    else:\n",
    "        raise ValueError(\"value_pick must be 'first' or 'last' or 'mean'\")\n",
    "\n",
    "    agg_funcs = {'VALUE': value_func}\n",
    "    features = chunk.groupby(['SUBJECT_ID', 'FEATURE_NAME']).agg(agg_funcs).unstack()\n",
    "\n",
    "    # Flatten Multi-Index columns\n",
    "    features.columns = [f'{i}_{j}' for i, j in features.columns]\n",
    "    return features.reset_index()\n",
    "    \n",
    "def filter_patients_within_timeframe(data, max_hours):\n",
    "\n",
    "    max_seconds = max_hours * 3600\n",
    "    # Mark patients to keep based on the condition\n",
    "    keep_patients = []\n",
    "    for i, row in data.iterrows():\n",
    "        if pd.notnull(row['DEATHTIME']):\n",
    "            time_diff = (row['DEATHTIME'] - row['ADMITTIME']).total_seconds()\n",
    "        else:\n",
    "            time_diff = (row['DISCHTIME'] - row['ADMITTIME']).total_seconds()\n",
    "        \n",
    "        if time_diff > max_seconds:\n",
    "            keep_patients.append(row['SUBJECT_ID'])\n",
    "    \n",
    "    return keep_patients\n",
    "    \n",
    "def process_data_with_timeframe_filtering(admit_times, discharge_times, death_times, filter_within_timeframe, max_hours, subject_ids):\n",
    "    death_labels = []\n",
    "    hospital_stay = []\n",
    "    filtered_ids = []\n",
    "    \n",
    "    for admit, dis, death, subject_id in tqdm.tqdm(zip(admit_times, discharge_times, death_times, subject_ids), total=len(admit_times), desc='Processing mortality'):\n",
    "        within_timeframe = False\n",
    "        death_time = (death - admit).total_seconds() if pd.notnull(death) else None\n",
    "        discharge_time = (dis - admit).total_seconds() if pd.notnull(dis) else None\n",
    "        \n",
    "        if death_time and death_time <= max_hours * 3600:\n",
    "            within_timeframe = True\n",
    "        elif discharge_time and discharge_time <= max_hours * 3600:\n",
    "            within_timeframe = True\n",
    "        \n",
    "        if filter_within_timeframe and within_timeframe:\n",
    "            continue  # Skip this patient\n",
    "        \n",
    "        # Add the patient's data\n",
    "        hospital_stay.append(death_time if death_time else discharge_time)\n",
    "        death_labels.append(1 if death_time else 0)\n",
    "        filtered_ids.append(subject_id)\n",
    "    \n",
    "    return death_labels, hospital_stay, filtered_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f68dd-dc5a-4baf-b4d3-ed56f6601cfb",
   "metadata": {},
   "source": [
    "# Model Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc17847a-9e60-4924-9df7-6b8467ea2cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for equalizing data\n",
    "def equalize_data(data, labels):\n",
    "    data[\"Death\"] = labels\n",
    "    data = data.sample(frac = 1) \n",
    "    true_data = data[data[\"Death\"] == 1]\n",
    "    false_data = data[data[\"Death\"] == 0]\n",
    "    true_length, false_length = len(true_data), len(false_data)\n",
    "    \n",
    "    # Equalizes labels\n",
    "    if true_length > false_length:\n",
    "        sample_fraction = false_length / true_length\n",
    "        true_data = true_data.sample(frac = sample_fraction)\n",
    "    else:\n",
    "        sample_fraction = true_length / false_length\n",
    "        false_data = false_data.sample(frac = sample_fraction)\n",
    "        \n",
    "    # Recombines data\n",
    "    data = pd.concat((true_data, false_data), axis = 0)\n",
    "    labels = list(data['Death'])\n",
    "    features = data.drop(columns = ['Death'])\n",
    "    return features, labels\n",
    "\n",
    "# Obtains all filtered features and patients\n",
    "def obtain_features(format_str = \"%Y-%m-%d %H:%M:%S\",\n",
    "                max_hours =6, patient_threshold = 0.5, feature_threshold = 0.5,\n",
    "                filter_within_timeframe = True):\n",
    "\n",
    "    # Sets global variables to facilitate multiprocessing functions\n",
    "    global id_conversion, num_hours\n",
    " \n",
    "    # Obtain discharge/death/admission times for label processing\n",
    "    data = pd.read_csv('dataset/ADMISSIONS.csv')\n",
    "    chart_data = pd.read_csv('dataset/CHARTEVENTS.csv',)\n",
    "    patient_data = pd.read_csv('dataset/PATIENTS.csv')[['SUBJECT_ID', 'DOB']]\n",
    "    patient_data['DOB'] = pd.to_datetime(patient_data['DOB'])\n",
    "\n",
    "    #micro_data = pd.read_csv('dataset/MICROBIOLOGYEVENTS.csv')\n",
    "    #note_data = pd.read_csv('./dataset/NOTEEVENTS.csv')\n",
    "    #prescription_data = pd.read_csv('dataset/PRESCRIPTIONS.csv')\n",
    "\n",
    "    # Conversion dict for item IDs \n",
    "    conversion_data = pd.read_csv('dataset/D_ITEMS.csv')\n",
    "\n",
    "    # Extract time-related discharge, death, and admit values and converts them to float\n",
    "    data['ADMITTIME'] = pd.to_datetime(data['ADMITTIME'],)\n",
    "    data['DISCHTIME'] = pd.to_datetime(data['DISCHTIME'])\n",
    "    data['DEATHTIME'] = pd.to_datetime(data['DEATHTIME'],)\n",
    "    \n",
    "    # New logic to filter patients based on discharge/death time within the timeframe\n",
    "    admit_times = data['ADMITTIME']\n",
    "    mapping_discharge = {data['SUBJECT_ID']: data['ADMITTIME'] for _, data in data.iterrows()}\n",
    "    discharge_times = data['DISCHTIME']\n",
    "    death_times = data['DEATHTIME']\n",
    "\n",
    "    admit_times = data['ADMITTIME']\n",
    "    discharge_times = data['DISCHTIME']\n",
    "    death_times = data['DEATHTIME']\n",
    "    \n",
    "    # Make sure 'ADMITTIME' and 'DOB' are in datetime format\n",
    "    data['ADMITTIME'] = pd.to_datetime(data['ADMITTIME'])\n",
    "    patient_data['DOB'] = pd.to_datetime(patient_data['DOB'])\n",
    "\n",
    "    # Calculate age using relativedelta\n",
    "    def calculate_age(row):\n",
    "        return dateutil.relativedelta.relativedelta(row['ADMITTIME'], row['DOB']).years\n",
    "\n",
    "    # Merge 'data' and 'patient_data' to get 'DOB' and 'ADMITTIME' in the same DataFrame for calculation\n",
    "    patient_age_df = data.merge(patient_data, on='SUBJECT_ID',)\n",
    "    patient_age_df['AGE'] = [None for _ in range(len(patient_age_df))]\n",
    "    patient_age_df.drop_duplicates(inplace = True)\n",
    "    \n",
    "    # Apply the age calculation\n",
    "    patient_age_df['AGE'] = patient_age_df.apply(calculate_age, axis=1)\n",
    "    patient_age_df = patient_age_df[['SUBJECT_ID', 'AGE']].copy()\n",
    "\n",
    "    # Filter based on discharge/death time within the timeframe and other logic...\n",
    "    death_labels, hospital_stay, filtered_ids = process_data_with_timeframe_filtering(\n",
    "        admit_times, discharge_times, death_times, filter_within_timeframe, max_hours, subject_ids = data['SUBJECT_ID'])\n",
    "\n",
    "    # Ensure filtered_ids is a list of IDs that should be kept\n",
    "    chart_data = chart_data[chart_data['SUBJECT_ID'].isin(filtered_ids)]\n",
    "    \n",
    "    mortality_labels = pd.DataFrame({\n",
    "        'SUBJECT_ID': filtered_ids,\n",
    "        'Death':death_labels\n",
    "    })\n",
    "    \n",
    "    # Convert ITEMID to biomedical statistic labels\n",
    "    id_conversion = {row['ITEMID']: row['LABEL'] for _, row in conversion_data.iterrows()}\n",
    "    \n",
    "    # Convert CHARTTIME to datetime\n",
    "    chart_data['CHARTTIME'] = pd.to_datetime(chart_data['CHARTTIME'])\n",
    "\n",
    "    # Convert times outside the loop\n",
    "    chart_data['CHARTTIME'] = pd.to_datetime(chart_data['CHARTTIME'])\n",
    "\n",
    "    # Properly adds admit times to df\n",
    "    print('Adding times...')\n",
    "    chart_admit = [mapping_discharge[subject_id] if subject_id in mapping_discharge else None for subject_id in chart_data['SUBJECT_ID']]\n",
    "    chart_data['admit_time'] = chart_admit\n",
    "    chart_data['admit_time'] = pd.to_datetime(chart_data['admit_time'], errors = 'coerce')\n",
    "    chart_data = chart_data.dropna(subset = ['admit_time'])\n",
    "\n",
    "    # Ensure datetime format and handle NaN values\n",
    "    chart_data['CHARTTIME'] = pd.to_datetime(chart_data['CHARTTIME'], errors='coerce')\n",
    "    chart_data = chart_data.dropna(subset=['CHARTTIME'])\n",
    "\n",
    "    # Filter chart_data to include only entries within the specified time frame\n",
    "    print('Filtering data...')\n",
    "    chart_data = filter_chart_data_optimized(chart_data, data, max_hours=max_hours)\n",
    "    \n",
    "    print('Obtaining features...')\n",
    "    feature_df = extract_features_from_chart_data(chart_data, id_conversion, num_processes=16)\n",
    "    \n",
    "    filtered_feature_df = filter_features_on_presence(feature_df, feature_threshold = feature_threshold, patient_threshold = patient_threshold)\n",
    "    \n",
    "    # Ensure SUBJECT_ID is a column if it's not already\n",
    "    patient_data_filtered = filtered_feature_df.reset_index(inplace=False)\n",
    "    \n",
    "    # Merge with mortality labels\n",
    "    patient_data = patient_data_filtered.merge(mortality_labels, on='SUBJECT_ID', how='left')\n",
    "\n",
    "    # Merge with ages\n",
    "    patient_age_df = patient_age_df.drop_duplicates()\n",
    "    patient_data = patient_age_df.merge(patient_data, on='SUBJECT_ID',)\n",
    "\n",
    "    # Drops columns with all NaN values and duplicate columns/rows\n",
    "    patient_data = patient_data.dropna(axis=1, how='all')\n",
    "\n",
    "    # Adds ICU Codes \n",
    "    coder = AutoEncoder()\n",
    "    encoded_features = coder.train_VAE(num_epochs = 5)\n",
    "    ICU_features = pd.DataFrame({\"SUBJECT_ID\":coder.subjects, \"VALUE_ICU_Code\":encoded_features})\n",
    "    patient_data = patient_data.merge(ICU_features, on = 'SUBJECT_ID')\n",
    "    \n",
    "    labellers = {}\n",
    "    string_columns_count = 0  # Counter for columns with string values\n",
    "    for column in tqdm.tqdm(patient_data.columns, total = len(patient_data.columns), desc = 'Labelling data...'):\n",
    "\n",
    "        # Check if there are any string values in the column\n",
    "        if patient_data[column].apply(lambda x: isinstance(x, str)).any():\n",
    "\n",
    "            try:\n",
    "                # Increment the counter as this column contains string data\n",
    "                string_columns_count += 1\n",
    "\n",
    "                # Fill NaN values with a placeholder\n",
    "                patient_data[column] = patient_data[column].fillna('missing')\n",
    "\n",
    "                # Initialize LabelEncoder and transform values\n",
    "                le = LabelEncoder()\n",
    "                patient_data[column] = le.fit_transform(patient_data[column])\n",
    "                labellers[column] = le\n",
    "\n",
    "                # Identify the encoded value for 'missing' placeholder\n",
    "                missing_label = le.transform(['missing'])[0]\n",
    "\n",
    "                # Replace the 'missing' encoded value with NaN in the dataframe\n",
    "                patient_data[column] = patient_data[column].replace(missing_label, None)\n",
    "            except:\n",
    "                patient_data[column] = patient_data[column].apply(lambda x: None if isinstance(x, str) else x)\n",
    "\n",
    "    # Number of filtered patients\n",
    "    print(f'{len(patient_data.columns)} features remaining after final filtration')\n",
    "    print(f'{len(patient_data)} patients remaining after final filtration')\n",
    "    \n",
    "    # Removes certain features \n",
    "    features_to_remove = ['VALUE_Education Learner', 'VALUE_Education Method', 'VALUE_Education Readiness', 'VALUE_Education Response', \n",
    "                            'Value_Marital Status', 'VALUE_Religion', 'VALUE_Orientation', 'VALUE_Family Communication', 'VALUE_Education Barrier',\n",
    "                            'VALUE_Code Status',]\n",
    "    \n",
    "    for feature in features_to_remove:\n",
    "        try:\n",
    "            patient_data = patient_data.drop(columns = [feature])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    labels = patient_data['Death']\n",
    "    features = patient_data.drop(columns = ['Death', 'SUBJECT_ID'])\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    numpy_features = np.array([[item for item in row] for row in features.to_numpy()])\n",
    "    numpy_features = scaler.fit_transform(numpy_features)\n",
    "    imputer = MICE()\n",
    "    \n",
    "    print('Imputing...')\n",
    "    impute_start = time.perf_counter()\n",
    "    imputed_data = imputer.fit_transform(numpy_features)\n",
    "    impute_end = time.perf_counter()\n",
    "    print(f'MICE Imputation finished in {round((impute_end - impute_start)/60, 4)} minutes!')\n",
    "    features = pd.DataFrame(imputed_data, columns=features.columns, index=features.index)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def feature_analysis(features, labels, top_features = None):\n",
    "    # Analyzes feature importance via a variety of methods and filters features if enabled\n",
    "    if top_features:\n",
    "        feature_names = analyze_features(features_scaled = features.to_numpy(), feature_names = list(features.columns), y = labels,\n",
    "                                    return_top_features = True, top_n_features = top_features,)\n",
    "        return feature_names\n",
    "    else:\n",
    "        analyze_features(features_scaled = features.to_numpy(), feature_names = list(features.columns), y = labels, return_top_features = False)\n",
    "    \n",
    "# Tests all models on the aggregated features and data\n",
    "def test_models(features, # Feature df = HAVE IT FILTERED  \n",
    "                labels, # Mortality labels\n",
    "                feature_names = None, # Feature names \n",
    "                equalize = True): # Equalize labels  \n",
    "\n",
    "    if feature_names != None:\n",
    "        features = features[feature_names]\n",
    "        \n",
    "    print(f'Number of features: {len(features.columns)}')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    def k_fold_validation_for_sklearn(clf, features, \n",
    "                                      labels, \n",
    "                                      name = 'Random Forest'):\n",
    "\n",
    "        print(f'Starting analysis for {name}')\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        for i, (train_index, test_index) in tqdm.tqdm(enumerate(kf.split(features)), total = n_splits):\n",
    "            \n",
    "            X_train, X_test = features[train_index], features[test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "            \n",
    "            # Scale features\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Fit and predict\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "           \n",
    "            # Compute ROC curve and AUC for this fold\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "            roc_auc = skauc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            \n",
    "            # Interpolate all ROC curves at this points\n",
    "            interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "            interp_tpr[0] = 0.0\n",
    "            plt.plot(mean_fpr, interp_tpr, alpha=0.4, label = f'split {i + 1} AUC = {round(roc_auc, 3)}')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        \n",
    "        # Calculate the mean AUC and std deviation\n",
    "        mean_auc = np.mean(aucs)\n",
    "        std_auc = np.std(aucs)\n",
    "        print(mean_auc)\n",
    "        plt.title(f'{name}, ROC (AUC = {mean_auc:.3f} $\\pm$ {std_auc:.3f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "        return clf\n",
    "\n",
    "    # Equalizes labels if enabled\n",
    "    if equalize == True:\n",
    "        features, labels = equalize_data(features, labels) \n",
    "    print(f'Number of processed data samples: {len(labels)}')\n",
    "    \n",
    "    # Prepare the data\n",
    "    features_array, labels = features.to_numpy(), np.array(labels)\n",
    "    \n",
    "    # Random Forest Classifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "    rf_clf = k_fold_validation_for_sklearn(rf_clf, features_array, labels)\n",
    "    \n",
    "    # XGBoost Classifier\n",
    "    xgb_clf = XGBClassifier(n_estimators=1000, learning_rate=0.05, use_label_encoder=False, eval_metric='logloss',\n",
    "                           name = 'XGBoost')\n",
    "    xgb_clf = k_fold_validation_for_sklearn(xgb_clf, features_array, labels, name = 'XGBoost')\n",
    "    \n",
    "    # Custom-built network\n",
    "    class NeuralNet(plit.LightningModule):\n",
    "        def __init__(self, input_size, hidden_size=32):\n",
    "            super(NeuralNet, self).__init__()\n",
    "            \n",
    "            self.network = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size * 8), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size * 8), nn.Dropout(0.3),\n",
    "                \n",
    "                nn.Linear(hidden_size * 8, hidden_size * 4), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size * 4), nn.Dropout(0.3),\n",
    "                \n",
    "                nn.Linear(hidden_size * 4, hidden_size * 2), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size * 2), nn.Dropout(0.3),\n",
    "                \n",
    "                nn.Linear(hidden_size * 2, hidden_size), nn.ReLU(),\n",
    "                \n",
    "                nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size), nn.Dropout(0.5),\n",
    "            )\n",
    "\n",
    "            self.final_linear = nn.Linear(hidden_size, 1)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.residual_transform = nn.Linear(input_size, hidden_size)\n",
    "            \n",
    "            # L1 and L2 regularization are defined but not used as layers\n",
    "            \n",
    "        def forward(self, x):\n",
    "            out = self.network(x)  # Pass input through the network layers\n",
    "            residual = self.residual_transform(x)  # Apply residual transformation to the original input\n",
    "            out += residual  # Add residual connection\n",
    "            out = self.final_linear(out)\n",
    "            out = self.sigmoid(out)\n",
    "            return out.squeeze()\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            inputs, labels = batch\n",
    "            outputs = self(inputs)\n",
    "            loss = nn.BCELoss()(outputs, labels.view(-1))  # BCELoss for binary classification\n",
    "\n",
    "            # Apply L1 and L2 regularization\n",
    "            l1_reg = torch.tensor(0.)\n",
    "            l2_reg = torch.tensor(0.)\n",
    "            for param in self.parameters():\n",
    "                l1_reg = l1_reg + torch.norm(param, 1)\n",
    "                l2_reg = l2_reg + torch.norm(param, 2)\n",
    "\n",
    "            # Regularization strengths need to be defined; for example:\n",
    "            lambda1, lambda2 = 0.005, 0.005\n",
    "            loss += lambda1 * l1_reg + lambda2 * l2_reg\n",
    "            self.log(\"train_loss\", loss)\n",
    "            return loss\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=0.001)\n",
    "            return optimizer\n",
    "    \n",
    "    def k_fold_validation_for_pytorch(model_class, features, labels, input_size):\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        plt.figure()\n",
    "        for train_index, test_index in tqdm.tqdm(kf.split(features), total = n_splits,):\n",
    "            # Prepare data\n",
    "            X_train, X_test = features[train_index], features[test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "           \n",
    "            # Convert to tensors\n",
    "            X_train_tensor, y_train_tensor = torch.FloatTensor(X_train), torch.FloatTensor(y_train)\n",
    "            X_test_tensor, y_test_tensor = torch.FloatTensor(X_test), torch.FloatTensor(y_test)\n",
    "            \n",
    "            # DataLoader setup\n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "            \n",
    "            # Model setup\n",
    "            model = model_class(input_size=input_size)\n",
    "            trainer = plit.Trainer(max_epochs=20)\n",
    "        \n",
    "            trainer.fit(model, train_loader)\n",
    "            \n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_prob = model(X_test_tensor).numpy()\n",
    "            \n",
    "            # Compute ROC curve and AUC for this fold\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "            roc_auc = skauc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            \n",
    "            interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "            interp_tpr[0] = 0.0\n",
    "            plt.plot(mean_fpr, interp_tpr, alpha=0.4)\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        mean_auc = np.mean(aucs)\n",
    "        std_auc = np.std(aucs)\n",
    "        plt.title(f'Custom Neural Network Mean ROC (AUC = {mean_auc:.3f} $\\pm$ {std_auc:.3f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    return rf_clf, xgb_clf, scaler\n",
    "    #print(\"Custom Neural Network\")\n",
    "    #k_fold_validation_for_pytorch(NeuralNet, features_array, labels, input_size=features.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273e9fd-d4e9-4d2e-a693-c354c4a23f1f",
   "metadata": {},
   "source": [
    "# Main Runtime + Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd5416-592b-4db0-ba8c-f284be66982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config options for mode\n",
    "hours = 24\n",
    "feature_threshold = 0.25\n",
    "patient_threshold = 0.1\n",
    "\n",
    "# Obtains features\n",
    "features, labels, feature_df = obtain_features(max_hours = hours, feature_threshold = feature_threshold,\n",
    "                                   patient_threshold = patient_threshold,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aeb823-45ac-42b6-ae5d-03f246d6989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs feature analysis\n",
    "feature_names = feature_analysis(features, labels, top_features = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b370d-d887-48f8-b4da-4463838de2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests model\n",
    "RF, XGB, scaler = test_models(features, labels, max_hours = hours, feature_names = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546aa26-4327-42c6-ad04-6f46a338d867",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1251e1be-acbc-4610-9c20-4ff9ace25908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_pipeline(feature_df, \n",
    "                       feature_drop,\n",
    "                       true_labels, \n",
    "                       model,\n",
    "                       subject_id,\n",
    "                       feature_columns, \n",
    "                       feature_vector = None):\n",
    "    \"\"\"\n",
    "    A pipeline to perform inference for a given patient by imputing missing features\n",
    "    based on similar patients in the training dataset.\n",
    "\n",
    "    :param feature_df: Path to the feature dataframe or  file.\n",
    "    :param model: A scikit-learn model or the path to a saved model file.\n",
    "    :param subject_id: The subject ID for which to perform the inference.\n",
    "    :param feature_columns: List of features expected by the model.\n",
    "    :return: The prediction for the given subject_id.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        feature_df.columns\n",
    "    except:\n",
    "        feature_df = pd.read_csv(feature_df)\n",
    "\n",
    "    # Obtains a random subject from df if no feature vector\n",
    "    if subject_id:\n",
    "        # Populates fake subject IDs for security\n",
    "        if 'SUBJECT_ID' not in feature_df.columns:\n",
    "            subject_ids = [i for i in range(len(feature_df))]\n",
    "            feature_df['SUBJECT_ID'] = subject_ids\n",
    "    \n",
    "        # Sets subjectID if unknown \n",
    "        if subject_id == 'random':\n",
    "            subject_id = random.sample(list(set(feature_df['SUBJECT_ID'])), 1)[0]\n",
    "            \n",
    "        subject_features = feature_df[feature_df['SUBJECT_ID'] == subject_id]\n",
    "        label_index = int(subject_features.index.item())\n",
    "    \n",
    "        # Reduce to the relevant features, handling missing data\n",
    "        drop_features = int(feature_drop * len(feature_columns))\n",
    "        selected_feature_columns = random.sample(feature_columns, len(feature_columns) - drop_features)\n",
    "        feature_vector = subject_features[selected_feature_columns].dropna(axis=1)\n",
    "\n",
    "    missing_features = set(feature_columns) - set(feature_vector.columns)\n",
    "                               \n",
    "    # Use training data to find similar patients and sample missing features\n",
    "    knn = NearestNeighbors(n_neighbors=5).fit(feature_df[selected_feature_columns])\n",
    "    _, indices = knn.kneighbors(feature_vector)\n",
    "    similar_patients_indices = indices.flatten()\n",
    "    \n",
    "    for feature in missing_features:\n",
    "        similar_feature_values = feature_df.iloc[similar_patients_indices][feature].dropna()\n",
    "        if not similar_feature_values.empty:\n",
    "            try:\n",
    "                distribution = gaussian_kde(similar_feature_values)\n",
    "                sampled_value = distribution.resample(1)[0]\n",
    "            except:\n",
    "                sampled_value = np.mean(similar_feature_values.iloc[0])\n",
    "            feature_vector[feature] = sampled_value  # Assuming recent_features has one row for the subject\n",
    "            \n",
    "    # Prepare the feature vector for prediction\n",
    "    feature_vector = np.array([feature_vector[feature].iloc[0] for feature in feature_columns]).reshape(1, -1)\n",
    "    feature_vector = scaler.transform(feature_vector)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction_probability = model.predict_proba(feature_vector)\n",
    "    prediction = np.argmax(prediction_probability)\n",
    "\n",
    "    try:\n",
    "        if prediction == true_labels[label_index]:\n",
    "            return prediction, 1\n",
    "        else:\n",
    "            return prediction, 0\n",
    "    except:\n",
    "        return prediction_probability[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ba93c-17f9-43e6-8a56-12d5f400616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_drops = [0, 0.25, 0.5, 0.8]\n",
    "sample_numbers = [100, 300, 500, 1000]\n",
    "feature_vec = {}\n",
    "for drop in feature_drops:\n",
    "    feature_vec[drop] = []\n",
    "    for sample_num in tqdm.tqdm(sample_numbers, total = len(sample_numbers), desc = f'Drop {drop}'):\n",
    "        correct = 0\n",
    "        for i in range(sample_num):\n",
    "            prediction, value = inference_pipeline(feature_df = features,\n",
    "                                        true_labels = labels,\n",
    "                                        model = XGB, \n",
    "                                        subject_id = 'random', \n",
    "                                        feature_columns = feature_names,  \n",
    "                                        feature_drop = drop)\n",
    "            correct += value\n",
    "        feature_vec[drop].append(correct/sample_num)\n",
    "plt.figure()\n",
    "for drop, value in feature_vec.items():\n",
    "    plt.plot(sample_numbers, value, label = f'{drop}')\n",
    "plt.xlabel('Sample number')\n",
    "plt.legend()\n",
    "plt.ylabel('Probability correct')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17369e44-62c2-41c9-90e8-6bcb862d4b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
