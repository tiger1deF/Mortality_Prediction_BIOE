{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "import sys\n",
    "import pytorch_lightning as plit\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, normalize, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import auc as skauc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import argparse\n",
    "import dateutil\n",
    "import shap\n",
    "import re\n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "\n",
    "# K-fold validation\n",
    "n_splits = 5  \n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Prescriptions and Admissions File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "#Load Prescriptions CSV File into Pandas Dataframe \n",
    "prescriptions_df = pd.read_csv(\"dataset/PRESCRIPTIONS.csv\")\n",
    "#Load Admissions CSV File into  Dataframe \n",
    "admissions_df = pd.read_csv(\"dataset/ADMISSIONS.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conversion factors for dosage units\n",
    "conversion_factors = {\n",
    "    'mg': 1,\n",
    "    'mcg': 0.001,\n",
    "    'g': 1000,\n",
    "    'pill': 50,\n",
    "    'tab': 50,\n",
    "    'gm': 1000,\n",
    "    'mEq': 74.5,\n",
    "    'mL': 1,\n",
    "    'UNIT': 100,\n",
    "    'mcg/hr': 0.001,\n",
    "    'mg/hr': 1,\n",
    "}\n",
    "\n",
    "# Convert DOSE_UNIT_RX to conversion factors and multiply by DOSE_VAL_RX to get dose in mg\n",
    "prescriptions_df['DOSE_VAL_RX'] = pd.to_numeric(prescriptions_df['DOSE_VAL_RX'], errors='coerce').fillna(0)\n",
    "prescriptions_df['dose_mg'] = prescriptions_df['DOSE_VAL_RX'] * prescriptions_df['DOSE_UNIT_RX'].map(conversion_factors).fillna(1)\n",
    "\n",
    "# Group by unique patient ID and DRUG to count occurrences and sum dosages\n",
    "grouped_counts = prescriptions_df.groupby(['SUBJECT_ID', 'DRUG']).size().unstack(fill_value=0)\n",
    "grouped_dosages = prescriptions_df.groupby(['SUBJECT_ID', 'DRUG'])['dose_mg'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Concatenate the counts and dosage DataFrames, ensuring we keep the SUBJECT_ID as a column\n",
    "final_df = pd.concat([grouped_counts.add_suffix('_count'), grouped_dosages.add_suffix('_dose')], axis=1)\n",
    "\n",
    "# Reset the index to convert 'SUBJECT_ID' back to a regular column\n",
    "final_df.reset_index(inplace=True)\n",
    "\n",
    "# Add a column for the number of unique drugs taken per patient\n",
    "final_df['num_unique_drugs'] = (grouped_counts > 0).sum(axis=1)\n",
    "\n",
    "# Add a column indicating if any drugs were taken by a patient (1 if any drug taken, 0 otherwise)\n",
    "final_df['any_drug_taken'] = final_df['num_unique_drugs'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing mortality: 100%|██████████| 58976/58976 [00:00<00:00, 122868.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of admissions_df: 58976\n",
      "Length of final_df: 39363\n",
      "Length of merged_df: 51366\n"
     ]
    }
   ],
   "source": [
    "# Filters out labels within a given timeframe\n",
    "def process_data_with_timeframe_filtering(admit_times, discharge_times, death_times, filter_within_timeframe, max_hours, subject_ids):\n",
    "    death_labels = []\n",
    "    hospital_stay = []\n",
    "    filtered_ids = []\n",
    "    \n",
    "    for admit, dis, death, subject_id in tqdm.tqdm(zip(admit_times, discharge_times, death_times, subject_ids), total=len(admit_times), desc='Processing mortality'):\n",
    "        within_timeframe = False\n",
    "        death_time = (death - admit).total_seconds() if pd.notnull(death) else None\n",
    "        discharge_time = (dis - admit).total_seconds() if pd.notnull(dis) else None\n",
    "        \n",
    "        if death_time and death_time <= max_hours * 3600:\n",
    "            within_timeframe = True\n",
    "        elif discharge_time and discharge_time <= max_hours * 3600:\n",
    "            within_timeframe = True\n",
    "        \n",
    "        if filter_within_timeframe and within_timeframe:\n",
    "            continue  # Skip this patient\n",
    "        \n",
    "        # Add the patient's data\n",
    "        hospital_stay.append(death_time if death_time else discharge_time)\n",
    "        death_labels.append(1 if death_time else 0)\n",
    "        filtered_ids.append(subject_id)\n",
    "    \n",
    "    return death_labels, hospital_stay, filtered_ids\n",
    "\n",
    "#ADMISSIONS FILE \n",
    "admissions_df['ADMITTIME'] = pd.to_datetime(admissions_df['ADMITTIME'],)\n",
    "admissions_df['DISCHTIME'] = pd.to_datetime(admissions_df['DISCHTIME'])\n",
    "admissions_df['DEATHTIME'] = pd.to_datetime(admissions_df['DEATHTIME'],)\n",
    "    \n",
    "# New logic to filter patients based on discharge/death time within the timeframe\n",
    "admit_times = admissions_df['ADMITTIME']\n",
    "mapping_discharge = {data['SUBJECT_ID']: data['ADMITTIME'] for _, data in admissions_df.iterrows()}\n",
    "discharge_times = admissions_df['DISCHTIME']\n",
    "death_times = admissions_df['DEATHTIME']\n",
    "\n",
    "# Filter based on discharge/death time within the timeframe and other logic...\n",
    "death_labels, hospital_stay, filtered_ids = process_data_with_timeframe_filtering(\n",
    "    admit_times, discharge_times, death_times, False, 0, subject_ids = admissions_df['SUBJECT_ID'],)\n",
    "\n",
    "mortality_labels = pd.DataFrame({\n",
    "    'SUBJECT_ID': filtered_ids,\n",
    "    'mortality_label':death_labels\n",
    "})\n",
    "\n",
    "merged_df = final_df.merge(mortality_labels, on = 'SUBJECT_ID', how = 'left')\n",
    "    \n",
    "# Check for duplicate subject IDs in final_df\n",
    "duplicate_subject_ids_final = final_df[final_df.duplicated('SUBJECT_ID', keep=False)]\n",
    "\n",
    "# Check for duplicate subject IDs in merged_df\n",
    "duplicate_subject_ids_merged = merged_df[merged_df.duplicated('SUBJECT_ID', keep=False)]\n",
    "\n",
    "# Print the lengths of the original dataframes and the merged dataframe\n",
    "print(\"Length of admissions_df:\", len(admissions_df))\n",
    "print(\"Length of final_df:\", len(final_df))\n",
    "print(\"Length of merged_df:\", len(merged_df))\n",
    "\n",
    "merged_df = merged_df[merged_df['SUBJECT_ID'].isin(admissions_df['SUBJECT_ID'])]\n",
    "merged_df.drop_duplicates(subset='SUBJECT_ID', keep=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(features_scaled, feature_names, y, top_n_features=10, return_top_features=False):\n",
    "    \"\"\"\n",
    "    Analyze features using SHAP values, Linear Regression, and PCA,\n",
    "    select the top features from each method without duplicates, and visualize the results.\n",
    "\n",
    "    :param features_scaled: Scaled feature numpy array\n",
    "    :param feature_names: List of feature names\n",
    "    :param y: True labels\n",
    "    :param top_n_features: Number of top features to retain from each method\n",
    "    \"\"\"\n",
    "    # Linear Regression for feature importance\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(features_scaled, y)\n",
    "\n",
    "    # Principal Component Analysis\n",
    "    pca = PCA(n_components=len(feature_names))\n",
    "    pca.fit(features_scaled)\n",
    "    feature_importance_pca = pca.explained_variance_ratio_\n",
    "    top_features_pca = np.argsort(feature_importance_pca)[-top_n_features:]\n",
    "\n",
    "    # Linear Regression\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    indices_lr = np.argsort(np.abs(lr_model.coef_))[::-1][:top_n_features]\n",
    "    plt.title(\"Top Linear Regression Coefficients\")\n",
    "    plt.barh(range(top_n_features), np.abs(lr_model.coef_)[indices_lr], color='b', align='center')\n",
    "    plt.yticks(range(top_n_features), [feature_names[i] for i in indices_lr])\n",
    "    plt.xscale('log')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have largest values at the top\n",
    "\n",
    "    # PCA\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Top PCA Explained Variance Ratio\")\n",
    "    plt.barh(range(top_n_features), feature_importance_pca[top_features_pca], color='b', align='center')\n",
    "    plt.yticks(range(top_n_features), [feature_names[i] for i in top_features_pca])\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have largest values at the top\n",
    "\n",
    "    # Combine and remove duplicates\n",
    "    combined_feature_indices = np.unique(np.concatenate((indices_lr, top_features_pca)))\n",
    "    combined_feature_names = [feature_names[i] for i in combined_feature_indices]\n",
    "\n",
    "    print(f\"Number of selected features: {len(combined_feature_names)}\")\n",
    "    print(\"Selected features:\", combined_feature_names)\n",
    "\n",
    "    if return_top_features:\n",
    "        return combined_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n"
     ]
    }
   ],
   "source": [
    "# Temporarily remove \"SUBJECT_ID\" and \"mortality_label\" columns\n",
    "subject_id = merged_df[['SUBJECT_ID']]\n",
    "mortality_label = merged_df[['mortality_label']]\n",
    "data_without_labels = merged_df.drop(columns=['SUBJECT_ID', 'mortality_label'])\n",
    "\n",
    "# Continue with your processing logic, now without 'SUBJECT_ID' and 'mortality_label'\n",
    "# Ensure these columns are not included in any of your feature lists\n",
    "numeric_features = [col for col in data_without_labels.select_dtypes(include=['float64', 'int64']).columns]\n",
    "categorical_features = data_without_labels.select_dtypes(include=['object']).columns.tolist()\n",
    "binary_features = [col for col in data_without_labels.columns if data_without_labels[col].nunique() == 2]\n",
    "\n",
    "# Clean Numeric Features as before\n",
    "for feature in numeric_features:\n",
    "    data_without_labels[feature] = data_without_labels[feature].astype(str).str.replace(',', '').apply(lambda x: re.sub(r'(\\d+)-(\\d+)', r'\\2', x))\n",
    "    data_without_labels[feature] = pd.to_numeric(data_without_labels[feature], errors='coerce')\n",
    "\n",
    "# Handle NaN values\n",
    "data_without_labels.ffill(inplace=True)\n",
    "\n",
    "# Apply transformations\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first', sparse=False)\n",
    "\n",
    "scaled_numeric = pd.DataFrame(numeric_transformer.fit_transform(data_without_labels[numeric_features]), columns=numeric_features, index=data_without_labels.index)\n",
    "encoded_categorical = pd.DataFrame(categorical_transformer.fit_transform(data_without_labels[categorical_features]), columns=categorical_transformer.get_feature_names_out(), index=data_without_labels.index)\n",
    "binary_df = data_without_labels[binary_features]  # Assuming binary features don't need transformation\n",
    "\n",
    "# Reunite transformed sections into a single DataFrame\n",
    "processed_df = pd.concat([scaled_numeric, encoded_categorical, binary_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters for numeric features\n",
    "numeric_df = processed_df[numeric_features]\n",
    "cols = list(numeric_df.columns)\n",
    "for i in range(len(cols)):\n",
    "    cols[i] = \"\".join([i for i in cols[i] if i not in ['[', ']', ',', '>', '<']]).replace(\" \", '_')\n",
    "numeric_df.columns = cols\n",
    "\n",
    "numeric_df = pd.concat((mortality_label, numeric_df), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    numeric_df.drop(columns = ['SUBJECT_ID'], inplace = True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Now, processed_df is ready for further analysis or modeling\n",
    "features = numeric_df.copy().drop(columns = ['mortality_label'])\n",
    "numpy_features = np.array([[item for item in row] for row in features.to_numpy()])\n",
    "imputer = MICE()\n",
    "\n",
    "print('Imputing...')\n",
    "unlabelled_data = imputer.fit_transform(numpy_features)\n",
    "print('Starting data analysis...')\n",
    "analyze_features(features_scaled=unlabelled_data, \n",
    "                 feature_names=unlabelled_df.columns,\n",
    "                 y=numeric_df['mortality_label'], \n",
    "                 top_n_features=30, \n",
    "                 return_top_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for equalizing data\n",
    "def equalize_data(data, labels):\n",
    "    data[\"Death\"] = labels\n",
    "    data = data.sample(frac = 1) \n",
    "    true_data = data[data[\"Death\"] == 1]\n",
    "    false_data = data[data[\"Death\"] == 0]\n",
    "    true_length, false_length = len(true_data), len(false_data)\n",
    "    \n",
    "    # Equalizes labels\n",
    "    if true_length > false_length:\n",
    "        sample_fraction = false_length / true_length\n",
    "        true_data = true_data.sample(frac = sample_fraction)\n",
    "    else:\n",
    "        sample_fraction = true_length / false_length\n",
    "        false_data = false_data.sample(frac = sample_fraction)\n",
    "        \n",
    "    # Recombines data\n",
    "    data = pd.concat((true_data, false_data), axis = 0)\n",
    "    labels = list(data['Death'])\n",
    "    features = data.drop(columns = ['Death'])\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def feature_analysis(features, labels, top_features = None):\n",
    "    # Analyzes feature importance via a variety of methods and filters features if enabled\n",
    "    if top_features:\n",
    "        feature_names = analyze_features(features_scaled = features.to_numpy(), feature_names = list(features.columns), y = labels,\n",
    "                                    return_top_features = True, top_n_features = top_features,)\n",
    "        return feature_names\n",
    "    else:\n",
    "        analyze_features(features_scaled = features.to_numpy(), feature_names = list(features.columns), y = labels, return_top_features = False)\n",
    "    \n",
    "# Tests all models on the aggregated features and data\n",
    "def test_models(features, # Feature df = HAVE IT FILTERED  \n",
    "                labels, # Mortality labels\n",
    "                feature_names = None, # Feature names \n",
    "                equalize = True,\n",
    "                rescale = True): # Equalize labels  \n",
    "\n",
    "    # Initializes scaler\n",
    "    scaler = StandardScaler()\n",
    "    if feature_names != None:\n",
    "        features = features[feature_names]\n",
    "        \n",
    "    print(f'Number of features: {len(features.columns)}')\n",
    "    \n",
    "    def k_fold_validation_for_sklearn(clf, features, labels, name='Random Forest'):\n",
    "        print(f'Starting analysis for {name}')\n",
    "        aucs = []\n",
    "        mccs = []  # List to store MCC scores\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "        for i, (train_index, test_index) in tqdm.tqdm(enumerate(kf.split(features)), total=n_splits):\n",
    "            \n",
    "            X_train, X_test = features[train_index], features[test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "            \n",
    "            # Scale features if required\n",
    "            if rescale == True:\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Fit and predict\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "            y_pred = clf.predict(X_test)  # Predictions for MCC\n",
    "    \n",
    "            # Compute ROC curve and AUC for this fold\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "            roc_auc = skauc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            \n",
    "            # Compute MCC for this fold\n",
    "            mcc = matthews_corrcoef(y_test, y_pred)\n",
    "            mccs.append(mcc)\n",
    "    \n",
    "            # Interpolate all ROC curves at this points\n",
    "            interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "            interp_tpr[0] = 0.0\n",
    "            plt.plot(mean_fpr, interp_tpr, alpha=0.4, label=f'Split {i + 1} AUC = {round(roc_auc, 3)}')\n",
    "    \n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "    \n",
    "        # Calculate the mean AUC, std deviation, mean MCC\n",
    "        mean_auc = np.mean(aucs)\n",
    "        std_auc = np.std(aucs)\n",
    "        mean_mcc = np.mean(mccs)  # Calculate mean MCC\n",
    "        print(f'Mean AUC: {mean_auc:.3f} ± {std_auc:.3f}')\n",
    "        print(f'Mean MCC: {mean_mcc:.3f}')\n",
    "        plt.title(f'{name}, ROC (AUC = {mean_auc:.3f} ± {std_auc:.3f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    \n",
    "        return clf\n",
    "    \n",
    "    # Equalizes labels if enabled\n",
    "    print(list(labels).count(1))\n",
    "    sys.exit()\n",
    "    if equalize == True:\n",
    "        features, labels = equalize_data(features, labels) \n",
    "\n",
    "    print(f'Number of processed data samples: {len(labels)}')\n",
    "    \n",
    "    # Prepare the data\n",
    "    features_array, labels = features.to_numpy(), np.array(labels)\n",
    "    \n",
    "    # Random Forest Classifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "    rf_clf = k_fold_validation_for_sklearn(rf_clf, features_array, labels)\n",
    "    \n",
    "    # XGBoost Classifier\n",
    "    xgb_clf = XGBClassifier(n_estimators=1000, learning_rate=0.05, use_label_encoder=False, eval_metric='logloss',\n",
    "                           name = 'XGBoost')\n",
    "    xgb_clf = k_fold_validation_for_sklearn(xgb_clf, features_array, labels, name = 'XGBoost')\n",
    "    \n",
    "    # Custom-built network\n",
    "    class NeuralNet(plit.LightningModule):\n",
    "        def __init__(self, input_size, hidden_size=32):\n",
    "            super(NeuralNet, self).__init__()\n",
    "            \n",
    "            self.network = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size * 8), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size * 8), nn.Dropout(0.3),\n",
    "                \n",
    "                nn.Linear(hidden_size * 8, hidden_size * 4), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size * 4), nn.Dropout(0.3),\n",
    "                \n",
    "                nn.Linear(hidden_size * 4, hidden_size * 2), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size * 2), nn.Dropout(0.3),\n",
    "                \n",
    "                nn.Linear(hidden_size * 2, hidden_size), nn.ReLU(),\n",
    "                \n",
    "                nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size), nn.Dropout(0.5),\n",
    "            )\n",
    "\n",
    "            self.final_linear = nn.Linear(hidden_size, 1)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.residual_transform = nn.Linear(input_size, hidden_size)\n",
    "            \n",
    "            # L1 and L2 regularization are defined but not used as layers\n",
    "            \n",
    "        def forward(self, x):\n",
    "            out = self.network(x)  # Pass input through the network layers\n",
    "            residual = self.residual_transform(x)  # Apply residual transformation to the original input\n",
    "            out += residual  # Add residual connection\n",
    "            out = self.final_linear(out)\n",
    "            out = self.sigmoid(out)\n",
    "            return out.squeeze()\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            inputs, labels = batch\n",
    "            outputs = self(inputs)\n",
    "            loss = nn.BCELoss()(outputs, labels.view(-1))  # BCELoss for binary classification\n",
    "\n",
    "            # Apply L1 and L2 regularization\n",
    "            l1_reg = torch.tensor(0.)\n",
    "            l2_reg = torch.tensor(0.)\n",
    "            for param in self.parameters():\n",
    "                l1_reg = l1_reg + torch.norm(param, 1)\n",
    "                l2_reg = l2_reg + torch.norm(param, 2)\n",
    "\n",
    "            # Regularization strengths need to be defined; for example:\n",
    "            lambda1, lambda2 = 0.005, 0.005\n",
    "            loss += lambda1 * l1_reg + lambda2 * l2_reg\n",
    "            self.log(\"train_loss\", loss)\n",
    "            return loss\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=0.001)\n",
    "            return optimizer\n",
    "    \n",
    "    def k_fold_validation_for_pytorch(model_class, features, labels, input_size):\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        plt.figure()\n",
    "        for train_index, test_index in tqdm.tqdm(kf.split(features), total = n_splits,):\n",
    "            # Prepare data\n",
    "            X_train, X_test = features[train_index], features[test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "           \n",
    "            # Convert to tensors\n",
    "            X_train_tensor, y_train_tensor = torch.FloatTensor(X_train), torch.FloatTensor(y_train)\n",
    "            X_test_tensor, y_test_tensor = torch.FloatTensor(X_test), torch.FloatTensor(y_test)\n",
    "            \n",
    "            # DataLoader setup\n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "            \n",
    "            # Model setup\n",
    "            model = model_class(input_size=input_size)\n",
    "            trainer = plit.Trainer(max_epochs=20)\n",
    "        \n",
    "            trainer.fit(model, train_loader)\n",
    "            \n",
    "            # Evaluate\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_prob = model(X_test_tensor).numpy()\n",
    "            \n",
    "            # Compute ROC curve and AUC for this fold\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "            roc_auc = skauc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            \n",
    "            interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "            interp_tpr[0] = 0.0\n",
    "            plt.plot(mean_fpr, interp_tpr, alpha=0.4)\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        mean_auc = np.mean(aucs)\n",
    "        std_auc = np.std(aucs)\n",
    "        plt.title(f'Custom Neural Network Mean ROC (AUC = {mean_auc:.3f} $\\pm$ {std_auc:.3f})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    return rf_clf, xgb_clf, scaler\n",
    "    #print(\"Custom Neural Network\")\n",
    "    #k_fold_validation_for_pytorch(NeuralNet, features_array, labels, input_size=features.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests models\n",
    "test_models(unlabelled_df, \n",
    "                merged_df['mortality_label'], # Mortality labels\n",
    "                feature_names = None, # Feature names \n",
    "                equalize = True,\n",
    "                rescale = True) # Equalize labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
