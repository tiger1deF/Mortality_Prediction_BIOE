{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "import sys\n",
    "import pytorch_lightning as plit\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, normalize, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, confusion_matrix, roc_curve\n",
    "from sklearn.metrics import auc as skauc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import argparse\n",
    "import dateutil\n",
    "import shap\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Prescriptions and Admissions File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Prescriptions CSV File into Pandas Dataframe \n",
    "prescriptions_df = pd.read_csv(\"C:/Users/calin/Desktop/PRESCRIPTIONS.csv\")\n",
    "#Load Admissions CSV File into  Dataframe \n",
    "admissions_df = pd.read_csv(\"C:/Users/calin/Desktop/ADMISSIONS.csv\",low_memory=False)\n",
    "\n",
    "\n",
    "print(\"Admissions DataFrame:\")\n",
    "print(admissions_df.head())\n",
    "print(\"\\nPrescriptions DataFrame:\")\n",
    "print(prescriptions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING NEW DATAFRAME AND DEFINING NEW FEATURES\n",
    "\n",
    "#DOSAGE AS A FEATURE \n",
    "conversion_factors = {\n",
    "\n",
    "    'mg': 1,\n",
    "    'mcg': 0.001,\n",
    "    'g' : 1000,\n",
    "    'pill': 50,\n",
    "    'tab': 50,\n",
    "    'gm': 1000,\n",
    "    'mEq': 74.5,\n",
    "    'mL': 1,\n",
    "    'UNIT': 100,\n",
    "    'mcg/hr':0.001,\n",
    "    'mg/hr':1,\n",
    "\n",
    "}\n",
    "\n",
    "def convert_to_mg(row):\n",
    "    unit = row['DOSE_UNIT_RX']\n",
    "    factor = conversion_factors.get(unit)\n",
    "    dose_val_rx = row['DOSE_VAL_RX']\n",
    "\n",
    "    if factor is not None and isinstance(dose_val_rx, (int,float)):\n",
    "        return dose_val_rx * factor\n",
    "    else: \n",
    "        return dose_val_rx #return original dose value\n",
    "    \n",
    "prescriptions_df['dose_mg'] = prescriptions_df.apply(convert_to_mg, axis=1)\n",
    "\n",
    "\n",
    "if prescriptions_df['dose_mg'].isnull().any():\n",
    "    prescriptions_df['dose_mg'] = prescriptions_df['dose_mg'].fillna(0)\n",
    "\n",
    "prescriptions_df['dose_mg'] = prescriptions_df.apply(convert_to_mg, axis=1)\n",
    "\n",
    "# Fill NaN values in 'dose_mg' with 0\n",
    "prescriptions_df['dose_mg'] = prescriptions_df['dose_mg'].fillna(0)\n",
    "\n",
    "#GROUP BY UNIQUE SUBJECT ID \n",
    "\n",
    "# Group by unique patient ID and pivot the table to have each drug as a separate column\n",
    "grouped_df = prescriptions_df.groupby(['SUBJECT_ID', 'DRUG']).size().unstack(fill_value=0)\n",
    "\n",
    "# Reset the index to convert 'SUBJECT_ID' back to a regular column\n",
    "grouped_df.reset_index(inplace=True)\n",
    "\n",
    "# Create a new DataFrame to store the final result\n",
    "final_df = grouped_df.copy()\n",
    "\n",
    "\n",
    "#EACH DRUG AS AN INDIVIDUAL FEATURE \n",
    "# Iterate over each column (drug) in the grouped DataFrame\n",
    "for drug in grouped_df.columns:\n",
    "    # Create a new column for the drug with binary indicators (0 or 1)\n",
    "    final_df[drug] = grouped_df[drug]\n",
    "    \n",
    "    # Create a new column for the dose of the drug if taken\n",
    "    final_df[f\"{drug.strip('_')}_dose\"] = prescriptions_df[prescriptions_df['DRUG'] == drug].groupby('SUBJECT_ID')['dose_mg'].sum()\n",
    "\n",
    "# Fill NaN values with 0 for drugs that were not taken\n",
    "final_df.fillna(0, inplace=True)\n",
    "\n",
    "# Add a column for the number of unique drugs taken\n",
    "final_df['num_unique_drugs'] = grouped_df.gt(0).sum(axis=1)\n",
    "\n",
    "# Add a column indicating if any drugs were taken (1 if any drug taken, 0 otherwise)\n",
    "final_df['any_drug_taken'] = final_df.iloc[:, :-1].any(axis=1).astype(int)\n",
    "\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADMISSIONS FILE \n",
    "\n",
    "admissions_df['ADMITTIME'] = pd.to_datetime(admissions_df['ADMITTIME'],)\n",
    "admissions_df['DISCHTIME'] = pd.to_datetime(admissions_df['DISCHTIME'])\n",
    "admissions_df['DEATHTIME'] = pd.to_datetime(admissions_df['DEATHTIME'],)\n",
    "    \n",
    "# New logic to filter patients based on discharge/death time within the timeframe\n",
    "admit_times = admissions_df['ADMITTIME']\n",
    "mapping_discharge = dict(zip(admissions_df['SUBJECT_ID'], admissions_df['ADMITTIME']))\n",
    "discharge_times = admissions_df['DISCHTIME']\n",
    "death_times = admissions_df['DEATHTIME']\n",
    "\n",
    "admit_times = admissions_df['ADMITTIME']\n",
    "discharge_times = admissions_df['DISCHTIME']\n",
    "death_times = admissions_df['DEATHTIME']\n",
    "# Make sure 'ADMITTIME' and 'DOB' are in datetime format\n",
    "admissions_df['ADMITTIME'] = pd.to_datetime(admissions_df['ADMITTIME'])\n",
    "\n",
    "\n",
    "#MERGING PRESCRIPTIONS WITH ADMISSIONS FILES \n",
    "merged_df = pd.merge(final_df, admissions_df, on='SUBJECT_ID', how='inner')\n",
    "\n",
    "print(\"\\nMergedDataframe:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "merged_df['mortality_label'] = 0 \n",
    "merged_df.loc[merged_df['dischtime'].notnull(), 'mortality_label'] = 0 #Patient who survived 0\n",
    "\n",
    "print(merged_df.head())\n",
    "\n",
    "duplicate_subject_ids_admissions = admissions_df[admissions_df.duplicated('SUBJECT_ID', keep=False)]\n",
    "\n",
    "# Check for duplicate subject IDs in final_df\n",
    "duplicate_subject_ids_final = final_df[final_df.duplicated('SUBJECT_ID', keep=False)]\n",
    "\n",
    "# Check for duplicate subject IDs in merged_df\n",
    "duplicate_subject_ids_merged = merged_df[merged_df.duplicated('SUBJECT_ID', keep=False)]\n",
    "\n",
    "# Print the lengths of the original dataframes and the merged dataframe\n",
    "print(\"Length of admissions_df:\", len(admissions_df))\n",
    "print(\"Length of final_df:\", len(final_df))\n",
    "print(\"Length of merged_df:\", len(merged_df))\n",
    "\n",
    "# Print the number of duplicate subject IDs in each dataframe\n",
    "print(\"Number of duplicate subject IDs in admissions_df:\", len(duplicate_subject_ids_admissions))\n",
    "print(\"Number of duplicate subject IDs in final_df:\", len(duplicate_subject_ids_final))\n",
    "print(\"Number of duplicate subject IDs in merged_df:\", len(duplicate_subject_ids_merged))\n",
    "\n",
    "# Optionally, print the duplicate subject IDs for further investigation\n",
    "print(\"Duplicate subject IDs in admissions_df:\", duplicate_subject_ids_admissions['SUBJECT_ID'].unique())\n",
    "print(\"Duplicate subject IDs in final_df:\", duplicate_subject_ids_final['SUBJECT_ID'].unique())\n",
    "print(\"Duplicate subject IDs in merged_df:\", duplicate_subject_ids_merged['SUBJECT_ID'].unique())\n",
    "\n",
    "\n",
    "\n",
    "merged_df = merged_df[merged_df['SUBJECT_ID'].isin(admissions_df['SUBJECT_ID'])]\n",
    "merged_df.drop_duplicates(subset='SUBJECT_ID', keep=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ANALYSIS + FILTRATION\n",
    "\n",
    "\n",
    "def analyze_features(features_scaled, # Feature dataframe\n",
    "                     feature_names, # Binary - on drug or not on drug EVERY DRUG NAME [i for i in features.columns if 'dose' in i] \n",
    "                     y, # Mortality labels \n",
    "                     top_n_features=10, # What features will be plotted (shap, LR, PCA) \n",
    "                     return_top_features = False ): # Return a list of filtered feature name  \n",
    "    \"\"\"\n",
    "    Analyze features using SHAP values, Linear Regression, and Random Forest, \n",
    "    select the top features from each method without duplicates, and visualize the results.\n",
    "\n",
    "    :param features_scaled: Scaled feature numpy array\n",
    "    :param feature_names: List of feature names\n",
    "    :param y: True labels\n",
    "    :param top_n_features: Number of top features to retain from each method\n",
    "    \"\"\"\n",
    "    # Trains XGBoost classifier for SHAP\n",
    "    xgb_model = XGBClassifier()\n",
    "    xgb_model.fit(features_scaled, y)\n",
    "    \n",
    "    # Trains Random Forest classifier\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(features_scaled, y)\n",
    "\n",
    "    # Linear Regression for feature importance\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(features_scaled, y)\n",
    "\n",
    "    # SHAP values\n",
    "    explainer_shap = shap.Explainer(xgb_model)\n",
    "    shap_values = explainer_shap(features_scaled)\n",
    "    sampled_features = shap.sample(features_scaled, 1000)\n",
    "    explainer_tree = shap.TreeExplainer(xgb_model)\n",
    "    shap_values_tree = explainer_tree.shap_values(sampled_features)\n",
    "\n",
    "    # Feature importance from SHAP\n",
    "    feature_importance_shap = np.abs(shap_values.values).mean(axis=0)\n",
    "    top_features_shap = np.argsort(feature_importance_shap)[-top_n_features:]\n",
    "\n",
    "    # Feature importance from RF\n",
    "    feature_importance_rf = rf_model.feature_importances_\n",
    "    top_features_rf = np.argsort(feature_importance_rf)[-top_n_features:]\n",
    "\n",
    "    # Feature importance from LR\n",
    "    feature_importance_lr = np.abs(lr_model.coef_)\n",
    "    top_features_lr = np.argsort(feature_importance_lr)[-top_n_features:]\n",
    "\n",
    "    # Visualization\n",
    "    \n",
    "    # Visualize SHAP values using summary plot\n",
    "    shap.summary_plot(shap_values_tree, sampled_features, feature_names=feature_names)\n",
    "\n",
    "    # Print feature importance based on SHAP\n",
    "    feature_importance_shap_tree = np.abs(shap_values_tree).mean(axis=0)\n",
    "    feature_importance_shap_tree = pd.Series(feature_importance_shap_tree, index=feature_names).sort_values(ascending=False)\n",
    "    print(\"\\nFeature importance based on SHAP TreeExplainer:\")\n",
    "    print(feature_importance_shap_tree.head(top_n_features))\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    # RF\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    indices = np.argsort(feature_importance_rf)[::-1][:top_n_features]\n",
    "    plt.title(\"Top Random Forest Feature Importances\")\n",
    "    plt.barh(range(top_n_features), feature_importance_rf[indices], color='b', align='center')\n",
    "    plt.yticks(range(top_n_features), [feature_names[i] for i in indices])\n",
    "    plt.xscale('log')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have largest values at the top\n",
    "\n",
    "    # LR\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    indices = np.argsort(feature_importance_lr)[::-1][:top_n_features]\n",
    "    plt.title(\"Top Linear Regression Coefficients\")\n",
    "    plt.barh(range(top_n_features), feature_importance_lr[indices], color='b', align='center')\n",
    "    plt.yticks(range(top_n_features), [feature_names[i] for i in indices])\n",
    "    plt.xscale('log')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have largest values at the top\n",
    "\n",
    "    # Combine and remove duplicates\n",
    "    combined_feature_indices = np.unique(np.concatenate((top_features_shap, top_features_rf, top_features_lr)))\n",
    "    combined_feature_names = [feature_names[i] for i in combined_feature_indices]\n",
    "\n",
    "    print(f\"Number of selected features: {len(combined_feature_names)}\")\n",
    "    print(\"Selected features:\", combined_feature_names)\n",
    "\n",
    "    if return_top_features:\n",
    "        return combined_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing & Scaling\n",
    "    \n",
    "#March 15\n",
    "\n",
    "# Separate numeric and categorical features    \n",
    "numeric_features = merged_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = merged_df.select_dtypes(include=['object']).columns.tolist()\n",
    "binary_features = [col for col in merged_df.columns if merged_df[col].nunique() == 2]\n",
    "\n",
    "# Remove 'mortality_label' from the selected features\n",
    "selected_features = [col for col in numeric_features + categorical_features + binary_features if col != 'mortality_label']\n",
    "\n",
    "# Filter explore_df to include only selected features\n",
    "explore_df = merged_df[selected_features].copy()\n",
    "\n",
    "# Clean numeric features\n",
    "explore_df[numeric_features] = explore_df[numeric_features].applymap(lambda x: str(x).replace(',', '') if isinstance(x, str) else x)\n",
    "explore_df[numeric_features] = explore_df[numeric_features].applymap(lambda x: re.sub(r'(\\d+)-(\\d+)', r'\\2', str(x)) if isinstance(x, str) else x)\n",
    "\n",
    "# Initialize transformers for preprocessing\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first')  # Dropping first category to avoid multicollinearity\n",
    "\n",
    "# Initialize an empty DataFrame to store transformed data\n",
    "X_processed = pd.DataFrame()\n",
    "\n",
    "# Iterate through each column\n",
    "for col in selected_features:\n",
    "    # Apply StandardScaler to numeric columns\n",
    "    if col in numeric_features:\n",
    "        X_processed[col] = numeric_transformer.fit_transform(merged_df[[col]])\n",
    "    # OneHotEncode categorical columns\n",
    "    elif col in categorical_features:\n",
    "        encoded_col = categorical_transformer.fit_transform(merged_df[[col]]).toarray()\n",
    "        feature_names = [f'{col}_{i}' for i in range(encoded_col.shape[1])]\n",
    "        X_processed[feature_names] = encoded_col\n",
    "    # Leave binary columns unchanged\n",
    "    elif col in binary_features:\n",
    "        X_processed[col] = merged_df[col]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),  # Apply StandardScaler to numeric features\n",
    "        ('cat', categorical_transformer, categorical_features),  # OneHotEncode categorical features\n",
    "        ('binary', 'passthrough', binary_features)  # Leave binary features unchanged\n",
    "    ],\n",
    "    remainder='drop'  # Drop any columns not explicitly transformed\n",
    ")\n",
    "\n",
    "\n",
    "# Extract the feature names after preprocessing\n",
    "numeric_feature_names = numeric_features\n",
    "\n",
    "\n",
    "categorical_feature_names = []\n",
    "\n",
    "for name, transformer, columns in preprocessor.transformers:\n",
    "    if 'cat' in name:\n",
    "        if isinstance(transformer, OneHotEncoder):\n",
    "            try:\n",
    "                feature_names = transformer.get_feature_names_out(columns)\n",
    "                # Ensure feature names are unique\n",
    "                unique_feature_names = [f\"{name}_{i}\" for i in range(len(feature_names))]\n",
    "                categorical_feature_names.extend(unique_feature_names)\n",
    "            except AttributeError:\n",
    "                # Handle the case when OneHotEncoder is not fitted yet\n",
    "                pass\n",
    "\n",
    "# Fit and transform the features\n",
    "X_processed = preprocessor.fit_transform(explore_df)\n",
    "feature_names = numeric_feature_names + categorical_feature_names + binary_features\n",
    "\n",
    "# Create a DataFrame with the scaled features\n",
    "features_scaled = pd.DataFrame(X_processed, columns=feature_names)\n",
    "\n",
    "# Now you can call your analyze_features function with the scaled features and mortality labels\n",
    "analyze_features(features_scaled=features_scaled, feature_names=feature_names, y=merged_df['mortality_label'], top_n_features=10, return_top_features=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
